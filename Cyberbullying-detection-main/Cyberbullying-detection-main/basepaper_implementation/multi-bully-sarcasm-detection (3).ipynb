{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-31T15:10:33.539074Z","iopub.execute_input":"2024-10-31T15:10:33.539396Z","iopub.status.idle":"2024-10-31T15:10:33.545179Z","shell.execute_reply.started":"2024-10-31T15:10:33.539363Z","shell.execute_reply":"2024-10-31T15:10:33.544267Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"! pip install -q gdown\n! pip install -q torchmetrics\n\n!wget --no-check-certificate -O bully_data.zip \"https://iitgoffice-my.sharepoint.com/:u:/g/personal/sjana_iitg_ac_in/EWdMrS9zHgBHnz4TTckw14kB14O8j0IbR_D-fBowyw7T7A?e=8H4Wpw&download=1\"\n\n!gdown \"https://drive.google.com/uc?id=1RpvrH0OJXBnJDzAFerqBb7v1ZKLbxTG8\"\n!gdown \"https://drive.google.com/uc?id=19oLWMCuVHye63YvKBrG7-EbW4OU_VQOQ\"\n\n!rm -rf bully_data/\n!mkdir bully_data\n!mkdir bully_data/data\n#!mkdir bully_data/test\n!unzip bully_data.zip -d bully_data/data/ | tqdm >/dev/null\n!unzip Cyberbully_corrected_emotion_sentiment_v2.zip -d bully_data/ | tqdm >/dev/null","metadata":{"execution":{"iopub.status.busy":"2024-11-24T13:58:07.065882Z","iopub.execute_input":"2024-11-24T13:58:07.066236Z","iopub.status.idle":"2024-11-24T13:58:51.739380Z","shell.execute_reply.started":"2024-11-24T13:58:07.066203Z","shell.execute_reply":"2024-11-24T13:58:51.738198Z"},"trusted":true},"outputs":[{"name":"stdout","text":"--2024-11-24 13:58:25--  https://iitgoffice-my.sharepoint.com/:u:/g/personal/sjana_iitg_ac_in/EWdMrS9zHgBHnz4TTckw14kB14O8j0IbR_D-fBowyw7T7A?e=8H4Wpw&download=1\nResolving iitgoffice-my.sharepoint.com (iitgoffice-my.sharepoint.com)... 13.107.138.10, 13.107.136.10, 2620:1ec:8f8::10, ...\nConnecting to iitgoffice-my.sharepoint.com (iitgoffice-my.sharepoint.com)|13.107.138.10|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: /personal/sjana_iitg_ac_in/Documents/bully_data-20240404T103912Z-001.zip?ga=1 [following]\n--2024-11-24 13:58:26--  https://iitgoffice-my.sharepoint.com/personal/sjana_iitg_ac_in/Documents/bully_data-20240404T103912Z-001.zip?ga=1\nReusing existing connection to iitgoffice-my.sharepoint.com:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 124138703 (118M) [application/x-zip-compressed]\nSaving to: 'bully_data.zip'\n\nbully_data.zip      100%[===================>] 118.39M  25.6MB/s    in 8.5s    \n\n2024-11-24 13:58:35 (14.0 MB/s) - 'bully_data.zip' saved [124138703/124138703]\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1RpvrH0OJXBnJDzAFerqBb7v1ZKLbxTG8\nTo: /kaggle/working/Cyberbully_corrected_emotion_sentiment.zip\n100%|█████████████████████████████████████████| 411k/411k [00:00<00:00, 106MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=19oLWMCuVHye63YvKBrG7-EbW4OU_VQOQ\nTo: /kaggle/working/Cyberbully_corrected_emotion_sentiment_v2.zip\n100%|████████████████████████████████████████| 422k/422k [00:00<00:00, 95.3MB/s]\n6008it [00:01, 5193.72it/s]\n2it [00:00, 54471.48it/s]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Download the Data\n\n## Images","metadata":{}},{"cell_type":"markdown","source":"## Excel Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport pickle\nimport os\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom torchvision import transforms, models\nfrom transformers import RobertaTokenizer, RobertaModel\n\nfrom torch.utils.data import random_split\nfrom sklearn.metrics import accuracy_score, f1_score\n\nimport numpy as np\nfrom PIL import Image\n\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-11-24T13:58:51.741441Z","iopub.execute_input":"2024-11-24T13:58:51.741714Z","iopub.status.idle":"2024-11-24T13:58:56.612865Z","shell.execute_reply.started":"2024-11-24T13:58:51.741687Z","shell.execute_reply":"2024-11-24T13:58:56.611893Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load the Excel file\n#df = pd.read_excel('/kaggle/working/bully_data/Cyberbully_corrected_emotion_sentiment_v2.xlsx')\ndf = pd.read_excel(\"/kaggle/working/bully_data/data/bully_data/Copy of Cyberbully_corrected_emotion_sentiment.xlsx\")\ndf = df.drop(columns=['Unnamed: 10', 'Unnamed: 11'])\n# Display the first few rows to ensure it is loaded correctly\n# print(df)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-24T13:58:56.614076Z","iopub.execute_input":"2024-11-24T13:58:56.614659Z","iopub.status.idle":"2024-11-24T13:58:58.038260Z","shell.execute_reply.started":"2024-11-24T13:58:56.614616Z","shell.execute_reply":"2024-11-24T13:58:58.037295Z"},"trusted":true},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"  Img_Name                                           Img_Text Img_Text_Label  \\\n0    0.jpg  Shivam @shivamishraa Girls be named naina and ...          Bully   \n1    1.jpg  Aaloo ke paranthe is the best breakfast Omelet...       Nonbully   \n2    2.jpg     For Boyfriend For Bestfriend DESI ADUKT TROLLS          Bully   \n3    3.jpg  You find a new YouTuber He's funny All of his ...       Nonbully   \n4    4.jpg  not_shubham14 @mentally_dank Kids at Marine Dr...          Bully   \n\n  Img_Label Text_Label Sentiment   Emotion Sarcasm      Harmful_Score  \\\n0  Nonbully      Bully  Negative   Disgust     Yes  Partially-Harmful   \n1  Nonbully   Nonbully   Neutral     Other      No           Harmless   \n2     Bully   Nonbully  Negative  Ridicule      No  Partially-Harmful   \n3  Nonbully   Nonbully   Neutral   Sadness      No           Harmless   \n4  Nonbully   Nonbully  Negative   Sadness      No  Partially-Harmful   \n\n       Target  \n0  Individual  \n1         NaN  \n2     Society  \n3         NaN  \n4  Individual  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Img_Name</th>\n      <th>Img_Text</th>\n      <th>Img_Text_Label</th>\n      <th>Img_Label</th>\n      <th>Text_Label</th>\n      <th>Sentiment</th>\n      <th>Emotion</th>\n      <th>Sarcasm</th>\n      <th>Harmful_Score</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.jpg</td>\n      <td>Shivam @shivamishraa Girls be named naina and ...</td>\n      <td>Bully</td>\n      <td>Nonbully</td>\n      <td>Bully</td>\n      <td>Negative</td>\n      <td>Disgust</td>\n      <td>Yes</td>\n      <td>Partially-Harmful</td>\n      <td>Individual</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.jpg</td>\n      <td>Aaloo ke paranthe is the best breakfast Omelet...</td>\n      <td>Nonbully</td>\n      <td>Nonbully</td>\n      <td>Nonbully</td>\n      <td>Neutral</td>\n      <td>Other</td>\n      <td>No</td>\n      <td>Harmless</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.jpg</td>\n      <td>For Boyfriend For Bestfriend DESI ADUKT TROLLS</td>\n      <td>Bully</td>\n      <td>Bully</td>\n      <td>Nonbully</td>\n      <td>Negative</td>\n      <td>Ridicule</td>\n      <td>No</td>\n      <td>Partially-Harmful</td>\n      <td>Society</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.jpg</td>\n      <td>You find a new YouTuber He's funny All of his ...</td>\n      <td>Nonbully</td>\n      <td>Nonbully</td>\n      <td>Nonbully</td>\n      <td>Neutral</td>\n      <td>Sadness</td>\n      <td>No</td>\n      <td>Harmless</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.jpg</td>\n      <td>not_shubham14 @mentally_dank Kids at Marine Dr...</td>\n      <td>Bully</td>\n      <td>Nonbully</td>\n      <td>Nonbully</td>\n      <td>Negative</td>\n      <td>Sadness</td>\n      <td>No</td>\n      <td>Partially-Harmful</td>\n      <td>Individual</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"df_cleaned = df.dropna()\ndf=df_cleaned\n# print(df)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-24T13:58:58.040641Z","iopub.execute_input":"2024-11-24T13:58:58.041415Z","iopub.status.idle":"2024-11-24T13:58:58.058765Z","shell.execute_reply.started":"2024-11-24T13:58:58.041385Z","shell.execute_reply":"2024-11-24T13:58:58.058075Z"},"trusted":true},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"  Img_Name                                           Img_Text Img_Text_Label  \\\n0    0.jpg  Shivam @shivamishraa Girls be named naina and ...          Bully   \n2    2.jpg     For Boyfriend For Bestfriend DESI ADUKT TROLLS          Bully   \n4    4.jpg  not_shubham14 @mentally_dank Kids at Marine Dr...          Bully   \n5    5.jpg                  what if we use 100% of our brain?          Bully   \n6    6.jpg  If the opposite of Con is Pro Is Congress the ...          Bully   \n\n  Img_Label Text_Label Sentiment   Emotion Sarcasm      Harmful_Score  \\\n0  Nonbully      Bully  Negative   Disgust     Yes  Partially-Harmful   \n2     Bully   Nonbully  Negative  Ridicule      No  Partially-Harmful   \n4  Nonbully   Nonbully  Negative   Sadness      No  Partially-Harmful   \n5     Bully   Nonbully  Negative  Surprise      No  Partially-Harmful   \n6  Nonbully      Bully  Negative   Sadness      No  Partially-Harmful   \n\n         Target  \n0    Individual  \n2       Society  \n4    Individual  \n5    Individual  \n6  Organization  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Img_Name</th>\n      <th>Img_Text</th>\n      <th>Img_Text_Label</th>\n      <th>Img_Label</th>\n      <th>Text_Label</th>\n      <th>Sentiment</th>\n      <th>Emotion</th>\n      <th>Sarcasm</th>\n      <th>Harmful_Score</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.jpg</td>\n      <td>Shivam @shivamishraa Girls be named naina and ...</td>\n      <td>Bully</td>\n      <td>Nonbully</td>\n      <td>Bully</td>\n      <td>Negative</td>\n      <td>Disgust</td>\n      <td>Yes</td>\n      <td>Partially-Harmful</td>\n      <td>Individual</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.jpg</td>\n      <td>For Boyfriend For Bestfriend DESI ADUKT TROLLS</td>\n      <td>Bully</td>\n      <td>Bully</td>\n      <td>Nonbully</td>\n      <td>Negative</td>\n      <td>Ridicule</td>\n      <td>No</td>\n      <td>Partially-Harmful</td>\n      <td>Society</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.jpg</td>\n      <td>not_shubham14 @mentally_dank Kids at Marine Dr...</td>\n      <td>Bully</td>\n      <td>Nonbully</td>\n      <td>Nonbully</td>\n      <td>Negative</td>\n      <td>Sadness</td>\n      <td>No</td>\n      <td>Partially-Harmful</td>\n      <td>Individual</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5.jpg</td>\n      <td>what if we use 100% of our brain?</td>\n      <td>Bully</td>\n      <td>Bully</td>\n      <td>Nonbully</td>\n      <td>Negative</td>\n      <td>Surprise</td>\n      <td>No</td>\n      <td>Partially-Harmful</td>\n      <td>Individual</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6.jpg</td>\n      <td>If the opposite of Con is Pro Is Congress the ...</td>\n      <td>Bully</td>\n      <td>Nonbully</td>\n      <td>Bully</td>\n      <td>Negative</td>\n      <td>Sadness</td>\n      <td>No</td>\n      <td>Partially-Harmful</td>\n      <td>Organization</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-11-24T13:58:58.059664Z","iopub.execute_input":"2024-11-24T13:58:58.059957Z","iopub.status.idle":"2024-11-24T13:58:58.083605Z","shell.execute_reply.started":"2024-11-24T13:58:58.059930Z","shell.execute_reply":"2024-11-24T13:58:58.082813Z"},"trusted":true},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 3105 entries, 0 to 5864\nData columns (total 10 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   Img_Name        3105 non-null   object\n 1   Img_Text        3105 non-null   object\n 2   Img_Text_Label  3105 non-null   object\n 3   Img_Label       3105 non-null   object\n 4   Text_Label      3105 non-null   object\n 5   Sentiment       3105 non-null   object\n 6   Emotion         3105 non-null   object\n 7   Sarcasm         3105 non-null   object\n 8   Harmful_Score   3105 non-null   object\n 9   Target          3105 non-null   object\ndtypes: object(10)\nmemory usage: 266.8+ KB\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-11-24T13:58:58.084540Z","iopub.execute_input":"2024-11-24T13:58:58.084859Z","iopub.status.idle":"2024-11-24T13:58:58.118251Z","shell.execute_reply.started":"2024-11-24T13:58:58.084818Z","shell.execute_reply":"2024-11-24T13:58:58.117532Z"},"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"        Img_Name                                           Img_Text  \\\ncount       3105                                               3105   \nunique      3105                                               3036   \ntop     6005.jpg  a head diaper is required when you have shit f...   \nfreq           1                                                  6   \n\n       Img_Text_Label Img_Label Text_Label Sentiment  Emotion Sarcasm  \\\ncount            3105      3105       3105      3105     3105    3105   \nunique              2         2          2         3       10       2   \ntop             Bully  Nonbully   Nonbully  Negative  Disgust      No   \nfreq             3076      2427       1640      2310      865    1577   \n\n            Harmful_Score      Target  \ncount                3105        3105  \nunique                  3           4  \ntop     Partially-Harmful  Individual  \nfreq                 3062        2430  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Img_Name</th>\n      <th>Img_Text</th>\n      <th>Img_Text_Label</th>\n      <th>Img_Label</th>\n      <th>Text_Label</th>\n      <th>Sentiment</th>\n      <th>Emotion</th>\n      <th>Sarcasm</th>\n      <th>Harmful_Score</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3105</td>\n      <td>3105</td>\n      <td>3105</td>\n      <td>3105</td>\n      <td>3105</td>\n      <td>3105</td>\n      <td>3105</td>\n      <td>3105</td>\n      <td>3105</td>\n      <td>3105</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>3105</td>\n      <td>3036</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>10</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>6005.jpg</td>\n      <td>a head diaper is required when you have shit f...</td>\n      <td>Bully</td>\n      <td>Nonbully</td>\n      <td>Nonbully</td>\n      <td>Negative</td>\n      <td>Disgust</td>\n      <td>No</td>\n      <td>Partially-Harmful</td>\n      <td>Individual</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>6</td>\n      <td>3076</td>\n      <td>2427</td>\n      <td>1640</td>\n      <td>2310</td>\n      <td>865</td>\n      <td>1577</td>\n      <td>3062</td>\n      <td>2430</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"print(\"Columns: \", df.columns.tolist())\nprint(f\"{df.columns.tolist()[2]}: {df[df.columns.tolist()[2]].unique()} \")\nprint(f\"{df.columns.tolist()[3]}: {df[df.columns.tolist()[3]].unique()} \")\nprint(f\"{df.columns.tolist()[4]}: {df[df.columns.tolist()[4]].unique()} \")\nprint(f\"{df.columns.tolist()[5]}: {df[df.columns.tolist()[5]].unique()} \")\nprint(f\"{df.columns.tolist()[6]}: {df[df.columns.tolist()[6]].unique()} \")\nprint(f\"{df.columns.tolist()[7]}: {df[df.columns.tolist()[7]].unique()} \")\nprint(f\"{df.columns.tolist()[8]}: {df[df.columns.tolist()[8]].unique()} \")\nprint(f\"{df.columns.tolist()[9]}: {df[df.columns.tolist()[9]].unique()} \")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-24T13:58:58.119152Z","iopub.execute_input":"2024-11-24T13:58:58.119377Z","iopub.status.idle":"2024-11-24T13:58:58.128500Z","shell.execute_reply.started":"2024-11-24T13:58:58.119354Z","shell.execute_reply":"2024-11-24T13:58:58.127625Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Columns:  ['Img_Name', 'Img_Text', 'Img_Text_Label', 'Img_Label', 'Text_Label', 'Sentiment', 'Emotion', 'Sarcasm', 'Harmful_Score', 'Target']\nImg_Text_Label: ['Bully' 'Nonbully'] \nImg_Label: ['Nonbully' 'Bully'] \nText_Label: ['Bully' 'Nonbully'] \nSentiment: ['Negative' 'Neutral' 'Positive'] \nEmotion: ['Disgust' 'Ridicule' 'Sadness' 'Surprise' 'Angry' 'Anticipation'\n 'Happiness' 'Other' 'Trust' 'Fear'] \nSarcasm: ['Yes' 'No'] \nHarmful_Score: ['Partially-Harmful' 'Very-Harmful' 'Harmless'] \nTarget: ['Individual' 'Society' 'Organization' 'Community'] \n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"df.iloc[761]","metadata":{"execution":{"iopub.status.busy":"2024-11-24T09:23:37.941420Z","iopub.execute_input":"2024-11-24T09:23:37.941753Z","iopub.status.idle":"2024-11-24T09:23:37.955089Z","shell.execute_reply.started":"2024-11-24T09:23:37.941714Z","shell.execute_reply":"2024-11-24T09:23:37.954316Z"},"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Img_Name                                                1098.png\nImg_Text          show me your bare naked ankles you dirty whore\nImg_Text_Label                                             Bully\nImg_Label                                               Nonbully\nText_Label                                                 Bully\nSentiment                                               Negative\nEmotion                                                 Surprise\nSarcasm                                                       No\nHarmful_Score                                  Partially-Harmful\nTarget                                                Individual\nName: 1098, dtype: object"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Assuming your DataFrame is called df and contains the 'img_id' column\n# Assuming image paths are in a directory (img_dir) and filenames correspond to 'img_id'\n\nimg_dir = \"/kaggle/working/bully_data/data/bully_data/\"\n\n# Define a function to check if the image size is zero\ndef is_zero_size(img_id, img_dir):\n    img_path = os.path.join(img_dir, img_id)\n    return os.path.exists(img_path) and os.path.getsize(img_path) == 0\n\n# Filter out rows with zero-size images\ndf['is_zero_size'] = df['Img_Name'].apply(lambda img_id: is_zero_size(img_id, img_dir))\ndf_filtered = df[df['is_zero_size'] == False].drop(columns='is_zero_size')\n\n# Now, df_filtered contains only rows with non-zero-size images\n# print(df_filtered)\ndf=df_filtered\ndf_cleaned = df[df['Img_Name'] != '2644.jpg']\ndf=df_cleaned\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-24T13:58:58.129568Z","iopub.execute_input":"2024-11-24T13:58:58.129794Z","iopub.status.idle":"2024-11-24T13:58:58.184806Z","shell.execute_reply.started":"2024-11-24T13:58:58.129755Z","shell.execute_reply":"2024-11-24T13:58:58.184052Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"  Img_Name                                           Img_Text Img_Text_Label  \\\n0    0.jpg  Shivam @shivamishraa Girls be named naina and ...          Bully   \n2    2.jpg     For Boyfriend For Bestfriend DESI ADUKT TROLLS          Bully   \n4    4.jpg  not_shubham14 @mentally_dank Kids at Marine Dr...          Bully   \n5    5.jpg                  what if we use 100% of our brain?          Bully   \n6    6.jpg  If the opposite of Con is Pro Is Congress the ...          Bully   \n\n  Img_Label Text_Label Sentiment   Emotion Sarcasm      Harmful_Score  \\\n0  Nonbully      Bully  Negative   Disgust     Yes  Partially-Harmful   \n2     Bully   Nonbully  Negative  Ridicule      No  Partially-Harmful   \n4  Nonbully   Nonbully  Negative   Sadness      No  Partially-Harmful   \n5     Bully   Nonbully  Negative  Surprise      No  Partially-Harmful   \n6  Nonbully      Bully  Negative   Sadness      No  Partially-Harmful   \n\n         Target  \n0    Individual  \n2       Society  \n4    Individual  \n5    Individual  \n6  Organization  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Img_Name</th>\n      <th>Img_Text</th>\n      <th>Img_Text_Label</th>\n      <th>Img_Label</th>\n      <th>Text_Label</th>\n      <th>Sentiment</th>\n      <th>Emotion</th>\n      <th>Sarcasm</th>\n      <th>Harmful_Score</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.jpg</td>\n      <td>Shivam @shivamishraa Girls be named naina and ...</td>\n      <td>Bully</td>\n      <td>Nonbully</td>\n      <td>Bully</td>\n      <td>Negative</td>\n      <td>Disgust</td>\n      <td>Yes</td>\n      <td>Partially-Harmful</td>\n      <td>Individual</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.jpg</td>\n      <td>For Boyfriend For Bestfriend DESI ADUKT TROLLS</td>\n      <td>Bully</td>\n      <td>Bully</td>\n      <td>Nonbully</td>\n      <td>Negative</td>\n      <td>Ridicule</td>\n      <td>No</td>\n      <td>Partially-Harmful</td>\n      <td>Society</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.jpg</td>\n      <td>not_shubham14 @mentally_dank Kids at Marine Dr...</td>\n      <td>Bully</td>\n      <td>Nonbully</td>\n      <td>Nonbully</td>\n      <td>Negative</td>\n      <td>Sadness</td>\n      <td>No</td>\n      <td>Partially-Harmful</td>\n      <td>Individual</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5.jpg</td>\n      <td>what if we use 100% of our brain?</td>\n      <td>Bully</td>\n      <td>Bully</td>\n      <td>Nonbully</td>\n      <td>Negative</td>\n      <td>Surprise</td>\n      <td>No</td>\n      <td>Partially-Harmful</td>\n      <td>Individual</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6.jpg</td>\n      <td>If the opposite of Con is Pro Is Congress the ...</td>\n      <td>Bully</td>\n      <td>Nonbully</td>\n      <td>Bully</td>\n      <td>Negative</td>\n      <td>Sadness</td>\n      <td>No</td>\n      <td>Partially-Harmful</td>\n      <td>Organization</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-24T13:58:58.186087Z","iopub.execute_input":"2024-11-24T13:58:58.186465Z","iopub.status.idle":"2024-11-24T13:58:58.192330Z","shell.execute_reply.started":"2024-11-24T13:58:58.186424Z","shell.execute_reply":"2024-11-24T13:58:58.191309Z"},"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(3078, 10)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# import os\n# import cv2\n# bad_list=[]\n# data_dir='/kaggle/working/bully_data/data/bully_data'\n# subdir_list=os.listdir(data_dir) # create a list of the sub directories in the directory ie train or test\n\n# for img_file in subdir_list:\n#     img_file_path = os.path.join(data_dir, img_file)\n#     index = img_file_path.rfind('.')\n#     file_ext = img_file_path[index+1:]\n    \n#     if file_ext not in ['jpg', 'png', 'bmp', 'gif', 'jpeg']:\n#         print(f'file {img_file_path}  has an invalid extension {file_ext}')\n#         bad_list.append(img_file)\n#     else:\n#         try:\n#             #print(img_file_path)\n#             #img = Image.open(img_file_path).convert('RGB')\n#             image = Image.open(\"/kaggle/working/bully_data/data/bully_data/81.jpg\").convert('RGB')\n#         except Exception as e:\n#             bad_list.append(img_file)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:08:51.243808Z","iopub.execute_input":"2024-10-31T15:08:51.244173Z","iopub.status.idle":"2024-10-31T15:08:51.254343Z","shell.execute_reply.started":"2024-10-31T15:08:51.244138Z","shell.execute_reply":"2024-10-31T15:08:51.253463Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n","metadata":{"execution":{"iopub.status.busy":"2024-11-24T09:23:38.012090Z","iopub.execute_input":"2024-11-24T09:23:38.012367Z","iopub.status.idle":"2024-11-24T09:23:38.021026Z","shell.execute_reply.started":"2024-11-24T09:23:38.012341Z","shell.execute_reply":"2024-11-24T09:23:38.020425Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Make Dataset","metadata":{}},{"cell_type":"code","source":"class MemeDataset(Dataset):\n    def __init__(self, dataframe, transform):\n        self.dataframe = dataframe\n        self.transform = transform\n        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n        self.img_folder = '/kaggle/working/bully_data/data/bully_data'\n        \n        # Define label mappings\n        self.text_label_mapping = {\n            \"Bully\": 1,\n            \"Nonbully\": 0\n        }\n        \n        self.sentiment_mapping = {\n            \"Positive\":1,\n            \"Neutral\": 0,\n            \"Negative\": 2\n        }\n        \n        self.emotion_mapping = {\n            \"Disgust\": 0,\n            \"Ridicule\": 1,\n            \"Sadness\": 2,\n            \"Surprise\": 3,\n            \"Anticipation\": 4,\n            \"Angry\": 5,\n            \"Happiness\": 6,\n            \"Other\": 7,\n            \"Trust\": 8,\n            \"Fear\": 9\n        }\n        \n        self.sarcasm_mapping = {\n            \"Yes\": 1,\n            \"No\": 0\n        }\n        \n        self.harmful_score_mapping = {\n            \"Harmless\": 0,\n            \"Partially-Harmful\": 1,\n            \"Very-Harmful\": 2\n        }\n        \n        self.target_mapping = {\n            \"Individual\": 0,\n            \"Society\": 1,\n            \"Organization\": 2,\n            \"Community\": 3\n        }\n    \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, idx):\n        # Load image\n        img_name = self.dataframe.iloc[idx]['Img_Name']\n        img_path = os.path.join(self.img_folder, img_name)\n        image = Image.open(img_path).convert('RGB')\n        image = self.transform(image)\n        \n        # Load and tokenize text\n        text = self.dataframe.iloc[idx]['Img_Text']\n        inputs = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n        \n        # Get labels and apply mappings\n        sentiment_label = torch.tensor(self.sentiment_mapping[self.dataframe.iloc[idx]['Sentiment']], dtype=torch.long)\n        emotion_label = torch.tensor(self.emotion_mapping[self.dataframe.iloc[idx]['Emotion']], dtype=torch.long)\n        sarcasm_label = torch.tensor(self.sarcasm_mapping[self.dataframe.iloc[idx]['Sarcasm']], dtype=torch.float)  # Binary sarcasm\n        bully_label = torch.tensor(self.text_label_mapping[self.dataframe.iloc[idx]['Img_Label']], dtype=torch.long)  # Bully detection\n        harmful_score_label = torch.tensor(self.harmful_score_mapping[self.dataframe.iloc[idx]['Harmful_Score']], dtype=torch.long)\n        target_label = torch.tensor(self.target_mapping[self.dataframe.iloc[idx]['Target']], dtype=torch.long)\n        \n        return image, inputs['input_ids'].squeeze(), inputs['attention_mask'].squeeze(), sentiment_label, emotion_label, sarcasm_label, bully_label, harmful_score_label, target_label","metadata":{"execution":{"iopub.status.busy":"2024-11-24T09:23:48.316666Z","iopub.execute_input":"2024-11-24T09:23:48.317405Z","iopub.status.idle":"2024-11-24T09:23:48.328530Z","shell.execute_reply.started":"2024-11-24T09:23:48.317368Z","shell.execute_reply":"2024-11-24T09:23:48.327765Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Load the dataset (assume the dataframe and image directory are available)\ndataset = MemeDataset(df, transform)\n\n# Create a DataLoader for batching\ndataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n\n# Iterate through the dataloader\nfor batch in dataloader:\n    ##print(f\"Len: {len(batch)}\") \n    # Further model training steps...\n    break\n    \n    \nprint(\"Done\")\n    \n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-10-31T15:08:51.282890Z","iopub.execute_input":"2024-10-31T15:08:51.283193Z","iopub.status.idle":"2024-10-31T15:08:51.681848Z","shell.execute_reply.started":"2024-10-31T15:08:51.283162Z","shell.execute_reply":"2024-10-31T15:08:51.680894Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Done\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"image = Image.open(\"/kaggle/working/bully_data/data/bully_data/81.jpg\").convert('RGB')","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:08:51.683421Z","iopub.execute_input":"2024-10-31T15:08:51.683878Z","iopub.status.idle":"2024-10-31T15:08:52.731064Z","shell.execute_reply.started":"2024-10-31T15:08:51.683832Z","shell.execute_reply":"2024-10-31T15:08:52.729558Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/bully_data/data/bully_data/81.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3498\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3496\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[1;32m   3497\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[0;32m-> 3498\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n","\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file '/kaggle/working/bully_data/data/bully_data/81.jpg'"],"ename":"UnidentifiedImageError","evalue":"cannot identify image file '/kaggle/working/bully_data/data/bully_data/81.jpg'","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# This Model Converges Faster\nclass MultiTaskModel(nn.Module):\n    def __init__(self):\n        super(MultiTaskModel, self).__init__()\n        \n        # Visual branch (CNN)\n        self.resnet = models.resnet50(pretrained=True)\n        self.resnet.fc = nn.Identity()  # Remove the final classification layer\n        \n        # Textual branch (RoBERTa)\n        self.roberta = RobertaModel.from_pretrained('roberta-base')\n        \n        # Shared fully connected layers\n        self.fc_shared = nn.Sequential(\n            nn.Linear(2048 + 768, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n        \n        # Task-specific heads\n        self.sentiment_head = nn.Linear(512, 3)  # Sentiment: 3 classes\n        self.emotion_head = nn.Linear(512, 10)    # Emotion: 6 classes, Its 10 not 6\n        self.sarcasm_head = nn.Linear(512, 1)    # Sarcasm: binary classification\n        self.bully_head = nn.Linear(512, 2)      # Cyberbullying: 2 classes\n        self.fc_harmful_score = nn.Linear(512, 3)  # 3 classes for harmful score\n        self.fc_target = nn.Linear(512, 4)  # 4 classes for target\n    \n    def forward(self, image, text_input_ids, text_attention_mask):\n        # Visual features\n        img_features = self.resnet(image)\n        \n        # Textual features\n        text_outputs = self.roberta(input_ids=text_input_ids, attention_mask=text_attention_mask)\n        text_features = text_outputs.pooler_output\n        \n        # Concatenate the visual and textual features\n        combined_features = torch.cat((img_features, text_features), dim=1)\n        \n        # Shared layers\n        shared_out = self.fc_shared(combined_features)\n        \n        # Task-specific outputs\n        sentiment_out = self.sentiment_head(shared_out)\n        emotion_out = self.emotion_head(shared_out)\n        sarcasm_out = self.sarcasm_head(shared_out)\n        bully_out = self.bully_head(shared_out)\n        harmful_out = self.fc_harmful_score(shared_out)\n        target_out = self.fc_target(shared_out)\n        \n        return sentiment_out, emotion_out, sarcasm_out, bully_out, harmful_out, target_out","metadata":{"execution":{"iopub.status.busy":"2024-11-24T09:24:12.180026Z","iopub.execute_input":"2024-11-24T09:24:12.180944Z","iopub.status.idle":"2024-11-24T09:24:12.188890Z","shell.execute_reply.started":"2024-11-24T09:24:12.180906Z","shell.execute_reply":"2024-11-24T09:24:12.187947Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class MemeModel(nn.Module):\n    def __init__(self):\n        super(MemeModel, self).__init__()\n        \n        # Load pre-trained RoBERTa model\n        self.text_model = RobertaModel.from_pretrained('roberta-base')\n        \n        # Load pre-trained ResNet model for image processing\n        self.image_model = models.resnet18(pretrained=True)\n        self.image_model.fc = nn.Linear(self.image_model.fc.in_features, 512)  # Modify the final layer\n        \n        # Shared fully connected layers\n        self.fc_shared = nn.Sequential(\n            nn.Linear(512 + 768, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n        )\n        \n        # Define fully connected layers for each task\n        self.fc_sentiment = nn.Linear(256, 3)  # 3 classes for sentiment\n        self.fc_emotion = nn.Linear(256, 10)  # 10 classes for emotion\n        self.fc_sarcasm = nn.Linear(256, 1)  # Binary classification for sarcasm\n        self.fc_bully = nn.Lineaavg_val_loss = val_loss_sum / len(val_dataloader)r(256, 2)  # Binary classification for bully detection\n        self.fc_harmful_score = nn.Linear(256, 3)  # 3 classes for harmful score\n        self.fc_target = nn.Linear(256, 4)  # 4 classes for target\n    \n    def forward(self, image, input_ids, attention_mask):\n        # Process text input\n        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n        text_features = text_outputs.last_hidden_state[:, 0, :]  # Use the [CLS] token representation\n        \n        # Process image input\n        image_features = self.image_model(image)\n        \n        # Concatenate text and image features\n        combined_features = torch.cat((text_features, image_features), dim=1)\n        \n        features = self.fc_shared(combined_features)\n        \n        # Pass through fully connected layers for each task\n        sentiment_output = self.fc_sentiment(features)\n        emotion_output = self.fc_emotion(features)\n        sarcasm_output = self.fc_sarcasm(features)\n        bully_output = self.fc_bully(features)\n        harmful_score_output = self.fc_harmful_score(features)\n        target_output = self.fc_target(features)\n        \n        # Task-specific outputs with softmax or sigmoid\n#         sentiment_output = F.softmax(self.fc_sentiment(features), dim=1)  # 3-class softmax\n#         emotion_output = F.softmax(self.fc_emotion(features), dim=1)      # 10-class softmax\n#         sarcasm_output = torch.sigmoid(self.fc_sarcasm(features))         # Binary sigmoid\n#         bully_output = F.softmax(self.fc_bully(features), dim=1)          # Binary softmax\n#         harmful_score_output = F.softmax(self.fc_harmful_score(features), dim=1)  # 3-class softmax\n#         target_output = F.softmax(self.fc_target(features), dim=1)        # 4-class softmax\n        \n        return sentiment_output, emotion_output, sarcasm_output, bully_output, harmful_score_output, target_output\n","metadata":{"execution":{"iopub.status.busy":"2024-11-24T09:24:19.963758Z","iopub.execute_input":"2024-11-24T09:24:19.964583Z","iopub.status.idle":"2024-11-24T09:24:19.974359Z","shell.execute_reply.started":"2024-11-24T09:24:19.964537Z","shell.execute_reply":"2024-11-24T09:24:19.973224Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[15], line 26\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.fc_bully = nn.Lineaavg_val_loss = val_loss_sum / len(val_dataloader)r(256, 2)  # Binary classification for bully detection\u001b[0m\n\u001b[0m                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (984173927.py, line 26)","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Example usage\nmodel = MemeModel()\nimage, input_ids, attention_mask, sentiment_label, emotion_label, sarcasm_label, bully_label, harmful_score_label, target_label = dataset[0]\nsentiment_output, emotion_output, sarcasm_output, bully_output, harmful_score_output, target_output = model(image.unsqueeze(0), input_ids.unsqueeze(0), attention_mask.unsqueeze(0))","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:13:34.279340Z","iopub.execute_input":"2024-10-31T15:13:34.279708Z","iopub.status.idle":"2024-10-31T15:13:35.165006Z","shell.execute_reply.started":"2024-10-31T15:13:34.279673Z","shell.execute_reply":"2024-10-31T15:13:35.163923Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:12:47.782479Z","iopub.execute_input":"2024-10-31T15:12:47.783483Z","iopub.status.idle":"2024-10-31T15:12:47.826091Z","shell.execute_reply.started":"2024-10-31T15:12:47.783431Z","shell.execute_reply":"2024-10-31T15:12:47.825096Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import torch.nn.functional as F\n\n# Define the loss functions\nloss_fn_sentiment = nn.CrossEntropyLoss()\nloss_fn_emotion = nn.CrossEntropyLoss()\nloss_fn_sarcasm = nn.BCEWithLogitsLoss()\nloss_fn_bully = nn.CrossEntropyLoss()\nloss_fn_harmful_score = nn.CrossEntropyLoss()\nloss_fn_target = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:12:50.333437Z","iopub.execute_input":"2024-10-31T15:12:50.333809Z","iopub.status.idle":"2024-10-31T15:12:50.339476Z","shell.execute_reply.started":"2024-10-31T15:12:50.333767Z","shell.execute_reply":"2024-10-31T15:12:50.338578Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# # Initialize the model\n# model = MemeModel()\n\n# # Define the optimizer\n# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:08:52.743087Z","iopub.status.idle":"2024-10-31T15:08:52.743894Z","shell.execute_reply.started":"2024-10-31T15:08:52.743645Z","shell.execute_reply":"2024-10-31T15:08:52.743671Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the dataset (assume the dataframe and image directory are available)\ndataset = MemeDataset(df, transform)\n\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\n\n# Split Dataset\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n# Create data loaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# Initialize the model and move it to the GPU\n\n# img_clip_inp_dim = 512\n# vgg_inp_dim = 25088\n# text_clip_inp_dim = 512\n# text_roberta_inp_dim = 768\n\n# Initialize the model\n#model = MemeModel()\nmodel = MultiTaskModel()\n\n# Parallelize\n# if torch.cuda.device_count() > 1:\n#     model = nn.DataParallel(model)  # Use DataParallel if multiple GPUs are available\n\nmodel.to(device) \n\n# Initialize the Optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:20:42.564880Z","iopub.execute_input":"2024-10-31T15:20:42.565824Z","iopub.status.idle":"2024-10-31T15:20:44.446893Z","shell.execute_reply.started":"2024-10-31T15:20:42.565770Z","shell.execute_reply":"2024-10-31T15:20:44.446092Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 171MB/s]\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"#Example usage\n#model = MemeModel()\nimage, input_ids, attention_mask, sentiment_label, emotion_label, sarcasm_label, bully_label, harmful_score_label, target_label = dataset[0]\nsentiment_output, emotion_output, sarcasm_output, bully_output, harmful_score_output, target_output = model(image, input_ids, attention_mask)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install -q torchmetrics\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score, BinaryAccuracy, BinaryF1Score\n\n# Initialize metrics\naccuracy_sentiment = MulticlassAccuracy(num_classes=3).to(device)\nf1_sentiment = MulticlassF1Score(num_classes=3, average=\"weighted\").to(device)\n\naccuracy_emotion = MulticlassAccuracy(num_classes=10).to(device)\nf1_emotion = MulticlassF1Score(num_classes=10, average=\"weighted\").to(device)\n\naccuracy_sarcasm = BinaryAccuracy().to(device)\nf1_sarcasm = BinaryF1Score().to(device)\n\n# accuracy_bully = BinaryAccuracy().to(device)\n# f1_bully = BinaryF1Score().to(device)\naccuracy_bully = MulticlassAccuracy(num_classes=2).to(device)\nf1_bully = MulticlassF1Score(num_classes=2, average=\"weighted\").to(device)\n\naccuracy_harmful_score = MulticlassAccuracy(num_classes=3).to(device)\nf1_harmful_score = MulticlassF1Score(num_classes=3, average=\"weighted\").to(device)\n\naccuracy_target = MulticlassAccuracy(num_classes=4).to(device)\nf1_target = MulticlassF1Score(num_classes=4, average=\"weighted\").to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T16:31:04.753876Z","iopub.execute_input":"2024-10-31T16:31:04.754234Z","iopub.status.idle":"2024-10-31T16:31:04.775949Z","shell.execute_reply.started":"2024-10-31T16:31:04.754202Z","shell.execute_reply":"2024-10-31T16:31:04.775184Z"},"trusted":true},"outputs":[],"execution_count":59},{"cell_type":"code","source":"def train_model(model, train_dataloader, val_dataloader, optimizer, losses, vals, num_epochs=10):\n    model.train()  # Set the model to training mode\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        model.train()\n        # batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n        batch_iterator = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f'Epoch [{epoch+1}/{num_epochs}]')\n        for batch_idx, (images, input_ids, attention_mask, sentiment_labels, emotion_labels, sarcasm_labels, bully_labels, harmful_score_labels, target_labels) in batch_iterator:\n            # Move data to the appropriate device (GPU or CPU)\n            images = images.to(device)\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            sentiment_labels = sentiment_labels.to(device)\n            emotion_labels = emotion_labels.to(device)\n            sarcasm_labels = sarcasm_labels.to(device)\n            bully_labels = bully_labels.to(device)\n            harmful_score_labels = harmful_score_labels.to(device)\n            target_labels = target_labels.to(device)\n\n            # Zero the parameter gradients\n            optimizer.zero_grad()\n\n            # Forward pass\n            sentiment_output, emotion_output, sarcasm_output, bully_output, harmful_score_output, target_output = model(images, input_ids, attention_mask)\n\n            # Compute the losses\n            loss_sentiment = loss_fn_sentiment(sentiment_output, sentiment_labels)\n            loss_emotion = loss_fn_emotion(emotion_output, emotion_labels)\n            loss_sarcasm = loss_fn_sarcasm(sarcasm_output.squeeze(), sarcasm_labels.float())\n            loss_bully = loss_fn_bully(bully_output, bully_labels)\n            loss_harmful_score = loss_fn_harmful_score(harmful_score_output, harmful_score_labels)\n            loss_target = loss_fn_target(target_output, target_labels)\n\n            # Combine the losses\n            loss = loss_sentiment + loss_emotion + loss_sarcasm + loss_bully + loss_harmful_score + loss_target\n\n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n\n            # Update running loss\n            running_loss += loss.item()\n            \n            # Calculate metrics\n            acc_sent = accuracy_sentiment(sentiment_output, sentiment_labels)\n            f1_sent = f1_sentiment(sentiment_output, sentiment_labels)\n            \n            acc_emo = accuracy_emotion(emotion_output, emotion_labels)\n            f1_emo = f1_emotion(emotion_output, emotion_labels)\n            \n            acc_sarc = accuracy_sarcasm(sarcasm_output.squeeze(), sarcasm_labels)\n            f1_sarc = f1_sarcasm(sarcasm_output.squeeze(), sarcasm_labels)\n            \n            acc_bully = accuracy_bully(bully_output, bully_labels)\n            f1_bully_metric = f1_bully(bully_output, bully_labels)\n            \n            acc_harm = accuracy_harmful_score(harmful_score_output, harmful_score_labels)\n            f1_harm = f1_harmful_score(harmful_score_output, harmful_score_labels)\n            \n            acc_target = accuracy_target(target_output, target_labels)\n            f1_target_metric = f1_target(target_output, target_labels)\n            \n            #tqdm.write(f\" Training Batch Loss: {loss.item():.4f}\")\n            # Update progress bar with current batch loss\n            #batch_iterator.set_postfix({'Train Batch Loss': loss.item(), 'Training Avg Loss': running_loss / (batch_idx + 1)})\n            #tqdm.set_postfix(batch_loss=loss.item(), avg_loss=running_loss / (len(data_sample) + 1))\n            # Update tqdm progress bar with current batch loss and metrics\n            batch_iterator.set_postfix({\n                'Train Loss': loss.item(),\n                'Acc Sent': acc_sent.item(),\n                'Acc Emo': acc_emo.item(),\n                'Acc Sarc': acc_sarc.item(),\n                'Acc Bully': acc_bully.item(),\n                'Acc Harm': acc_harm.item(),\n                'Acc Target': acc_target.item(),\n                'F1 Sent': f1_sent.item(),\n                'F1 Emo': f1_emo.item(),\n                'F1 Sarc': f1_sarc.item(),\n                'F1 Bully': f1_bully_metric.item(),\n                'F1 Harm': f1_harm.item(),\n                'F1 Target': f1_target_metric.item()\n            })\n\n        # Print the average loss for this epoch\n        epoch_loss = running_loss / len(train_dataloader)\n        losses.append(epoch_loss)\n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n        \n        # Validation\n        model.eval()\n        val_running_loss = 0.0\n    \n        with torch.no_grad():\n            # batch_iterator = tqdm(val_dataloader, desc=f\"Processing Validation {epoch:02d}\")\n            # Initialize accumulators for losses and metrics\n            val_loss_sum = 0.0\n            acc_sent_sum, f1_sent_sum = 0.0, 0.0\n            acc_emo_sum, f1_emo_sum = 0.0, 0.0\n            acc_sarc_sum, f1_sarc_sum = 0.0, 0.0\n            acc_bully_sum, f1_bully_sum = 0.0, 0.0\n            acc_harm_sum, f1_harm_sum = 0.0, 0.0\n            acc_target_sum, f1_target_sum = 0.0, 0.0\n            \n            accuracy_sentiment.reset()\n            f1_sentiment.reset()\n            accuracy_emotion.reset()\n            f1_emotion.reset()\n            accuracy_sarcasm.reset()\n            f1_sarcasm.reset()\n            accuracy_bully.reset()\n            f1_bully.reset()\n            accuracy_harmful_score.reset()\n            f1_harmful_score.reset()\n            accuracy_target.reset()\n            f1_target.reset()\n            \n            batch_iterator = tqdm(enumerate(val_dataloader), total=len(train_dataloader), desc=f'Epoch [{epoch+1}/{num_epochs}]')\n            for batch_idx, (images, input_ids, attention_mask, sentiment_labels, emotion_labels, sarcasm_labels, bully_labels, harmful_score_labels, target_labels) in batch_iterator:\n                #extract features\n                images = images.to(device)\n                input_ids = input_ids.to(device)\n                attention_mask = attention_mask.to(device)\n                sentiment_labels = sentiment_labels.to(device)\n                emotion_labels = emotion_labels.to(device)\n                sarcasm_labels = sarcasm_labels.to(device)\n                bully_labels = bully_labels.to(device)\n                harmful_score_labels = harmful_score_labels.to(device)\n                target_labels = target_labels.to(device)\n                \n                \n                # Forward pass\n                sentiment_output, emotion_output, sarcasm_output, bully_output, harmful_score_output, target_output = model(images, input_ids, attention_mask)\n\n                # Calculate Loss\n                # Compute the losses\n\n                loss_sentiment = loss_fn_sentiment(sentiment_output, sentiment_labels)\n                loss_emotion = loss_fn_emotion(emotion_output, emotion_labels)\n                loss_sarcasm = loss_fn_sarcasm(sarcasm_output.squeeze(), sarcasm_labels.float())\n                loss_bully = loss_fn_bully(bully_output, bully_labels)\n                loss_harmful_score = loss_fn_harmful_score(harmful_score_output, harmful_score_labels)\n                loss_target = loss_fn_target(target_output, target_labels)\n                \n                # Combine the losses\n                loss = loss_sentiment + loss_emotion + loss_sarcasm + loss_bully + loss_harmful_score + loss_target\n                \n                # Update running loss\n                val_running_loss += loss.item()\n                \n                # Calculate metrics\n                val_acc_sent = accuracy_sentiment(sentiment_output, sentiment_labels)\n                val_f1_sent = f1_sentiment(sentiment_output, sentiment_labels)\n                \n                val_acc_emo = accuracy_emotion(emotion_output, emotion_labels)\n                val_f1_emo = f1_emotion(emotion_output, emotion_labels)\n                \n                val_acc_sarc = accuracy_sarcasm(sarcasm_output.squeeze(), sarcasm_labels)\n                val_f1_sarc = f1_sarcasm(sarcasm_output.squeeze(), sarcasm_labels)\n                \n                val_acc_bully = accuracy_bully(bully_output, bully_labels)\n                val_f1_bully_metric = f1_bully(bully_output, bully_labels)\n                \n                val_acc_harm = accuracy_harmful_score(harmful_score_output, harmful_score_labels)\n                val_f1_harm = f1_harmful_score(harmful_score_output, harmful_score_labels)\n                \n                val_acc_target = accuracy_target(target_output, target_labels)\n                val_f1_target_metric = f1_target(target_output, target_labels)\n                \n                # Accumulate the metrics\n                acc_sent_sum += val_acc_sent.item()\n                f1_sent_sum += val_f1_sent.item()\n\n                acc_emo_sum += val_acc_emo.item()\n                f1_emo_sum += val_f1_emo.item()\n\n                acc_sarc_sum += val_acc_sarc.item()\n                f1_sarc_sum += val_f1_sarc.item()\n\n                acc_bully_sum += val_acc_bully.item()\n                f1_bully_sum += val_f1_bully_metric.item()\n\n                acc_harm_sum += val_acc_harm.item()\n                f1_harm_sum += val_f1_harm.item()\n\n                acc_target_sum += val_acc_target.item()\n                f1_target_sum += val_f1_target_metric.item()\n                \n                #tqdm.write(f\" Validation Batch Loss: {loss.item():.4f}\")\n                #batch_iterator.set_postfix({'Val Batch Loss': loss.item(), 'Val Avg Loss': val_running_loss / (batch_idx + 1)})\n                #tqdm.set_postfix(batch_loss=loss.item(), avg_loss=val_running_loss / (len(data_sample) + 1))\n                \n                # Update tqdm progress bar with validation metrics\n                batch_iterator.set_postfix({\n                    'Val Loss': loss.item(),\n                    'Val Acc Sent': val_acc_sent.item(),\n                    'Val Acc Emo': val_acc_emo.item(),\n                    'Val Acc Sarc': val_acc_sarc.item(),\n                    'Val Acc Bully': val_acc_bully.item(),\n                    'Val Acc Harm': val_acc_harm.item(),\n                    'Val Acc Target': val_acc_target.item(),\n                    'Val F1 Sent': val_f1_sent.item(),\n                    'Val F1 Emo': val_f1_emo.item(),\n                    'Val F1 Sarc': val_f1_sarc.item(),\n                    'Val F1 Bully': val_f1_bully_metric.item(),\n                    'Val F1 Harm': val_f1_harm.item(),\n                    'Val F1 Target': val_f1_target_metric.item()\n                })\n                \n            # Print the average loss for this epoch\n            val_running_loss = val_running_loss / len(val_dataloader)\n            vals.append(val_running_loss)\n            print(f'Epoch {epoch+1}/{num_epochs}, Val_Loss: {val_running_loss:.4f}')\n            \n            # Calculate averages for losses and metrics\n            num_batches = len(val_dataloader)\n            avg_val_loss = val_loss_sum / num_batches\n            avg_acc_sent = acc_sent_sum / num_batches\n            avg_f1_sent = f1_sent_sum / num_batches\n\n            avg_acc_emo = acc_emo_sum / num_batches\n            avg_f1_emo = f1_emo_sum / num_batches\n\n            avg_acc_sarc = acc_sarc_sum / num_batches\n            avg_f1_sarc = f1_sarc_sum / num_batches\n\n            avg_acc_bully = acc_bully_sum / num_batches\n            avg_f1_bully = f1_bully_sum / num_batches\n\n            avg_acc_harm = acc_harm_sum / num_batches\n            avg_f1_harm = f1_harm_sum / num_batches\n\n            avg_acc_target = acc_target_sum / num_batches\n            avg_f1_target = f1_target_sum / num_batches\n\n            # Log the averaged metrics\n            print(f'Validation Loss: {avg_val_loss:.4f}')\n            print(f'Acc Sent: {avg_acc_sent:.4f}, F1 Sent: {avg_f1_sent:.4f}')\n            print(f'Acc Emo: {avg_acc_emo:.4f}, F1 Emo: {avg_f1_emo:.4f}')\n            print(f'Acc Sarc: {avg_acc_sarc:.4f}, F1 Sarc: {avg_f1_sarc:.4f}')\n            print(f'Acc Bully: {avg_acc_bully:.4f}, F1 Bully: {avg_f1_bully:.4f}')\n            print(f'Acc Harm: {avg_acc_harm:.4f}, F1 Harm: {avg_f1_harm:.4f}')\n            print(f'Acc Target: {avg_acc_target:.4f}, F1 Target: {avg_f1_target:.4f}')\n            \n\n    print('Training complete')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T16:30:43.675151Z","iopub.execute_input":"2024-10-31T16:30:43.675500Z","iopub.status.idle":"2024-10-31T16:30:43.714556Z","shell.execute_reply.started":"2024-10-31T16:30:43.675467Z","shell.execute_reply":"2024-10-31T16:30:43.713562Z"},"trusted":true},"outputs":[],"execution_count":57},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Trainavg_val_loss = val_loss_sum / len(val_dataloader)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nlosses = []\nvals = []\ntrain_model(model, train_dataloader, val_dataloader, optimizer, losses, vals, num_epochs=25)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T16:03:09.064618Z","iopub.execute_input":"2024-10-31T16:03:09.065018Z","iopub.status.idle":"2024-10-31T16:09:18.035577Z","shell.execute_reply.started":"2024-10-31T16:03:09.064981Z","shell.execute_reply":"2024-10-31T16:09:18.034667Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Epoch [1/5]: 100%|██████████| 154/154 [00:52<00:00,  2.93it/s, Train Loss=0.57, Acc Sent=1, Acc Emo=0.5, Acc Sarc=1, Acc Bully=1, Acc Harm=1, Acc Target=0.667, F1 Sent=1, F1 Emo=0.524, F1 Sarc=1, F1 Bully=1, F1 Harm=1, F1 Target=0.7]                            \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Loss: 0.6598\n","output_type":"stream"},{"name":"stderr","text":"Epoch [1/5]: 100%|██████████| 154/154 [00:20<00:00,  7.38it/s, Val Loss=0.0367, Val Acc Sent=1, Val Acc Emo=1, Val Acc Sarc=1, Val Acc Bully=1, Val Acc Harm=1, Val Acc Target=1, Val F1 Sent=1, Val F1 Emo=1, Val F1 Sarc=1, Val F1 Bully=1, Val F1 Harm=1, Val F1 Target=1]                                    \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Val_Loss: 1.3094\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2/5]: 100%|██████████| 154/154 [00:52<00:00,  2.93it/s, Train Loss=0.523, Acc Sent=0.9, Acc Emo=0.958, Acc Sarc=1, Acc Bully=1, Acc Harm=1, Acc Target=1, F1 Sent=0.918, F1 Emo=0.958, F1 Sarc=1, F1 Bully=1, F1 Harm=1, F1 Target=1]                               \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Loss: 0.6391\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2/5]: 100%|██████████| 154/154 [00:21<00:00,  7.30it/s, Val Loss=0.81, Val Acc Sent=1, Val Acc Emo=0.944, Val Acc Sarc=0.786, Val Acc Bully=0.75, Val Acc Harm=1, Val Acc Target=1, Val F1 Sent=1, Val F1 Emo=0.933, Val F1 Sarc=0.571, Val F1 Bully=0.813, Val F1 Harm=1, Val F1 Target=1]                  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Val_Loss: 1.4363\n","output_type":"stream"},{"name":"stderr","text":"Epoch [3/5]: 100%|██████████| 154/154 [00:52<00:00,  2.92it/s, Train Loss=0.637, Acc Sent=1, Acc Emo=0.857, Acc Sarc=1, Acc Bully=1, Acc Harm=1, Acc Target=0.615, F1 Sent=1, F1 Emo=0.837, F1 Sarc=1, F1 Bully=1, F1 Harm=1, F1 Target=0.639]                           \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Loss: 0.6060\n","output_type":"stream"},{"name":"stderr","text":"Epoch [3/5]: 100%|██████████| 154/154 [00:21<00:00,  7.23it/s, Val Loss=0.0659, Val Acc Sent=1, Val Acc Emo=1, Val Acc Sarc=1, Val Acc Bully=1, Val Acc Harm=1, Val Acc Target=1, Val F1 Sent=1, Val F1 Emo=1, Val F1 Sarc=1, Val F1 Bully=1, Val F1 Harm=1, Val F1 Target=1]                               \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Val_Loss: 1.2224\n","output_type":"stream"},{"name":"stderr","text":"Epoch [4/5]: 100%|██████████| 154/154 [00:52<00:00,  2.91it/s, Train Loss=1.15, Acc Sent=0.63, Acc Emo=1, Acc Sarc=0.929, Acc Bully=0.75, Acc Harm=1, Acc Target=1, F1 Sent=0.593, F1 Emo=1, F1 Sarc=0.909, F1 Bully=0.788, F1 Harm=1, F1 Target=1]                  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Loss: 0.5846\n","output_type":"stream"},{"name":"stderr","text":"Epoch [4/5]: 100%|██████████| 154/154 [00:20<00:00,  7.38it/s, Val Loss=0.216, Val Acc Sent=1, Val Acc Emo=0.95, Val Acc Sarc=1, Val Acc Bully=1, Val Acc Harm=1, Val Acc Target=1, Val F1 Sent=1, Val F1 Emo=0.905, Val F1 Sarc=1, Val F1 Bully=1, Val F1 Harm=1, Val F1 Target=1]                                \n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Val_Loss: 1.1671\n","output_type":"stream"},{"name":"stderr","text":"Epoch [5/5]: 100%|██████████| 154/154 [00:52<00:00,  2.92it/s, Train Loss=0.407, Acc Sent=1, Acc Emo=0.75, Acc Sarc=0.929, Acc Bully=1, Acc Harm=1, Acc Target=0.464, F1 Sent=1, F1 Emo=0.75, F1 Sarc=0.933, F1 Bully=1, F1 Harm=1, F1 Target=0.481]                      \n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Loss: 0.5260\n","output_type":"stream"},{"name":"stderr","text":"Epoch [5/5]: 100%|██████████| 154/154 [00:21<00:00,  7.28it/s, Val Loss=0.0439, Val Acc Sent=1, Val Acc Emo=1, Val Acc Sarc=1, Val Acc Bully=1, Val Acc Harm=1, Val Acc Target=1, Val F1 Sent=1, Val F1 Emo=1, Val F1 Sarc=1, Val F1 Bully=1, Val F1 Harm=1, Val F1 Target=1]                                    ","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Val_Loss: 1.3024\nTraining complete\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"train_model(model, train_dataloader, val_dataloader, optimizer, losses, vals, num_epochs=1)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T16:31:08.788104Z","iopub.execute_input":"2024-10-31T16:31:08.788577Z","iopub.status.idle":"2024-10-31T16:32:06.620567Z","shell.execute_reply.started":"2024-10-31T16:31:08.788538Z","shell.execute_reply":"2024-10-31T16:32:06.619644Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Epoch [1/1]: 100%|██████████| 154/154 [00:52<00:00,  2.93it/s, Train Loss=0.812, Acc Sent=1, Acc Emo=0.833, Acc Sarc=0.857, Acc Bully=0.875, Acc Harm=0.464, Acc Target=1, F1 Sent=1, F1 Emo=0.85, F1 Sarc=0.9, F1 Bully=0.925, F1 Harm=0.963, F1 Target=1]          \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1, Loss: 0.4935\n","output_type":"stream"},{"name":"stderr","text":"Epoch [1/1]:  25%|██▌       | 39/154 [00:05<00:15,  7.41it/s, Val Loss=14.7, Val Acc Sent=0.417, Val Acc Emo=0.208, Val Acc Sarc=0.625, Val Acc Bully=0.375, Val Acc Harm=1, Val Acc Target=0.429, Val F1 Sent=0.577, Val F1 Emo=0.229, Val F1 Sarc=0.571, Val F1 Bully=0.857, Val F1 Harm=1, Val F1 Target=0.75]       ","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1, Val_Loss: 10.7649\nValidation Loss: 0.0000\nAcc Sent: 0.5438, F1 Sent: 0.6723\nAcc Emo: 0.1539, F1 Emo: 0.2223\nAcc Sarc: 0.5609, F1 Sarc: 0.5259\nAcc Bully: 0.6020, F1 Bully: 0.7168\nAcc Harm: 0.9060, F1 Harm: 0.9787\nAcc Target: 0.3822, F1 Target: 0.7033\nTraining complete\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"! pip install -q torchviz","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:44:27.553174Z","iopub.execute_input":"2024-10-31T15:44:27.553847Z","iopub.status.idle":"2024-10-31T15:44:41.669215Z","shell.execute_reply.started":"2024-10-31T15:44:27.553807Z","shell.execute_reply":"2024-10-31T15:44:41.667871Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from torchviz import make_dot\n\nmake_dot((sentiment_output, emotion_output, sarcasm_output, bully_output, harmful_score_output, target_output), params=dict(list(model.named_parameters()))).render(\"MultiTaskMeme_torchviz\", format=\"png\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T15:45:52.819451Z","iopub.execute_input":"2024-10-31T15:45:52.819875Z","iopub.status.idle":"2024-10-31T15:45:57.737141Z","shell.execute_reply.started":"2024-10-31T15:45:52.819823Z","shell.execute_reply":"2024-10-31T15:45:57.736212Z"},"trusted":true},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"'MultiTaskMeme_torchviz.png'"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Validation phase\nmodel.eval()  # Set model to evaluation mode\ntotal_val_loss = 0\nepoch = 27\n# Initialize lists to store true and predicted labels for each task\nall_labels_bully = []\nall_preds_bully = []\n\nall_labels_sentiment = []\nall_preds_sentiment = []\n\nall_labels_emotion = []\nall_preds_emotion = []\n\nall_labels_sarcasm = []\nall_preds_sarcasm = []\n\nall_labels_harmful_score = []\nall_preds_harmful_score = []\n\nall_labels_target = []\nall_preds_target = []\n\nwith torch.no_grad():  # Disable gradient calculation\n    for images, text_input_ids, text_attention_mask, sentiment_labels, emotion_labels, sarcasm_labels, bully_labels, harmful_score_labels, target_labels in val_dataloader:\n        # Move data to the GPU\n        images = images.to(device)\n        text_input_ids = text_input_ids.to(device)\n        text_attention_mask = text_attention_mask.to(device)\n        sentiment_labels = sentiment_labels.to(device)\n        emotion_labels = emotion_labels.to(device)\n        sarcasm_labels = sarcasm_labels.to(device)\n        bully_labels = bully_labels.to(device)\n        harmful_score_labels = harmful_score_labels.to(device)\n        target_labels = target_labels.to(device)\n\n        # Forward pass\n        sentiment_out, emotion_out, sarcasm_out, bully_out, harmful_score_out, target_out = model(images, text_input_ids, text_attention_mask)\n\n        # Compute loss for each task\n        loss_sentiment = loss_fn_sentiment(sentiment_out, sentiment_labels)\n        loss_emotion = loss_fn_emotion(emotion_out, emotion_labels)\n        loss_sarcasm = loss_fn_sarcasm(sarcasm_out.squeeze(), sarcasm_labels.float())\n        loss_bully = loss_fn_bully(bully_out, bully_labels)\n        loss_harmful_score = loss_fn_harmful_score(harmful_score_out, harmful_score_labels)\n        loss_target = loss_fn_target(target_out, target_labels)\n\n        # Total loss\n        total_val_loss += (loss_sentiment + loss_emotion + loss_sarcasm + loss_bully + loss_harmful_score + loss_target).item()\n\n        # Get predictions for each task\n        _, predicted_sentiment = torch.max(sentiment_out, 1)\n        _, predicted_emotion = torch.max(emotion_out, 1)\n        _, predicted_sarcasm = torch.max(sarcasm_out, 1)\n        _, predicted_bully = torch.max(bully_out, 1)\n        _, predicted_harmful_score = torch.max(harmful_score_out, 1)  # Assuming multi-class\n        _, predicted_target = torch.max(target_out, 1)  # Assuming multi-class\n\n        # Collect true and predicted labels for each task\n        all_labels_sentiment.append(sentiment_labels.cpu().numpy())\n        all_preds_sentiment.append(predicted_sentiment.cpu().numpy())\n\n        all_labels_emotion.append(emotion_labels.cpu().numpy())\n        all_preds_emotion.append(predicted_emotion.cpu().numpy())\n\n        all_labels_sarcasm.append(sarcasm_labels.cpu().numpy())\n        all_preds_sarcasm.append(predicted_sarcasm.cpu().numpy())\n\n        all_labels_bully.appenavg_val_loss = val_loss_sum / len(val_dataloader)d(bully_labels.cpu().numpy())\n        all_preds_bully.append(predicted_bully.cpu().numpy())\n\n        all_labels_harmful_score.append(harmful_score_labels.cpu().numpy())\n        all_preds_harmful_score.append(predicted_harmful_score.cpu().numpy())\n\n        all_labels_target.append(target_labels.cpu().numpy())\n        all_preds_target.append(predicted_target.cpu().numpy())\n\navg_val_loss = total_val_loss / len(val_dataloader)\n\n# Flatten lists for each task\nall_labels_bully = np.concatenate(all_labels_bully)\nall_preds_bully = np.concatenate(all_preds_bully)\n\nall_labels_sentiment = np.concatenate(all_labels_sentiment)\nall_preds_sentiment = np.concatenate(all_preds_sentiment)\n\nall_labels_emotion = np.concatenate(all_labels_emotion)\nall_preds_emotion = np.concatenate(all_preds_emotion)\n\nall_labels_sarcasm = np.concatenate(all_labels_sarcasm)\nall_preds_sarcasm = np.concatenate(all_preds_sarcasm)\n\nall_labels_harmful_score = np.concatenate(all_labels_harmful_score)\nall_preds_harmful_score = np.concatenate(all_preds_harmful_score)\n\nall_labels_target = np.concatenate(all_labels_target)\nall_preds_target = np.concatenate(all_preds_target)\n\n# Calculate accuracy and F1 score for each task\naccuracy_bully = accuracy_score(all_labels_bully, all_preds_bully)\nf1_bully = f1_score(all_labels_bully, all_preds_bully, average='weighted')\n\naccuracy_sentiment = accuracy_score(all_labels_sentiment, all_preds_sentiment)\nf1_sentiment = f1_score(all_labels_sentiment, all_preds_sentiment, average='weighted')\n\naccuracy_emotion = accuracy_score(all_labels_emotion, all_preds_emotion)\nf1_emotion = f1_score(all_labels_emotion, all_preds_emotion, average='weighted')\n\naccuracy_sarcasm = accuracy_score(all_labels_sarcasm, all_preds_sarcasm)\nf1_sarcasm = f1_score(all_labels_sarcasm, all_preds_sarcasm, average='weighted')\n\naccuracy_harmful_score = accuracy_score(all_labels_harmful_score, all_preds_harmful_score)\nf1_harmful_score = f1_score(all_labels_harmful_score, all_preds_harmful_score, average='weighted')\n\naccuracy_target = accuracy_score(all_labels_target, all_preds_target)\nf1_target = f1_score(all_labels_target, all_preds_target, average='weighted')\n\nprint(f'Epoch {epoch}, Validation Loss: {avg_val_loss:.4f},\\n'\n      f'Bully Accuracy: {accuracy_bully:.4f}, F1 Score: {f1_bully:.4f},\\n'\n      f'Sentiment Accuracy: {accuracy_sentiment:.4f}, F1 Score: {f1_sentiment:.4f},\\n'\n      f'Emotion Accuracy: {accuracy_emotion:.4f}, F1 Score: {f1_emotion:.4f},\\n'\n      f'Sarcasm Accuracy: {accuracy_sarcasm:.4f}, F1 Score: {f1_sarcasm:.4f},\\n'\n      f'Harmful Score Accuracy: {accuracy_harmful_score:.4f}, F1 Score: {f1_harmful_score:.4f},\\n'\n      f'Target Accuracy: {accuracy_target:.4f}, F1 Score: {f1_target:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T16:29:56.899575Z","iopub.execute_input":"2024-10-31T16:29:56.899971Z","iopub.status.idle":"2024-10-31T16:30:01.527297Z","shell.execute_reply.started":"2024-10-31T16:29:56.899937Z","shell.execute_reply":"2024-10-31T16:30:01.526260Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 27, Validation Loss: 11.0457,\nBully Accuracy: 0.7321, F1 Score: 0.7105,\nSentiment Accuracy: 0.6558, F1 Score: 0.6632,\nEmotion Accuracy: 0.2419, F1 Score: 0.2367,\nSarcasm Accuracy: 0.5016, F1 Score: 0.3351,\nHarmful Score Accuracy: 0.9838, F1 Score: 0.9786,\nTarget Accuracy: 0.7062, F1 Score: 0.6969\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Create the dataset and dataloader\ndataset = MemeDataset(df, transform=transform)\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n# Create data loaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# dataloader = DataLoader(dataset, batch_size=16, shuffle=True)  # Adjust batch size as needed\n\n# Initialize the model and move it to the GPU\nmodel = MultiTaskModel()\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)  # Use DataParallel if multiple GPUs are available\nmodel.to(device) \noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n# Loss functions\nloss_fn_sentiment = nn.CrossEntropyLoss().to(device)\nloss_fn_emotion = nn.CrossEntropyLoss().to(device)\nloss_fn_sarcasm = nn.BCEWithLogitsLoss().to(device)\nloss_fn_bully = nn.CrossEntropyLoss().to(device)\nloss_fn_harmful_score = nn.CrossEntropyLoss().to(device)\nloss_fn_target = nn.CrossEntropyLoss().to(device)\n\n# Training loop\nfor epoch in range(10):  # Set epochs accordingly\n    model.train()\n    \n    total_loss = 0  # Initialize total loss for the epoch\n    \n#     for images, text_input_ids, text_attention_mask, sentiment_labels, emotion_labels, sarcasm_labels, bully_labels, harmful_score_labels, target_labels in train_dataloader:\n        # Move data to the GPU\n    for images, text_input_ids, text_attention_mask, sentiment_labels, emotion_labels, sarcasm_labels, bully_labels, harmful_score_labels, target_labels in train_dataloader:\n  \n        images = images.to(device)\n        text_input_ids = text_input_ids.to(device)\n        text_attention_mask = text_attention_mask.to(device)\n        sentiment_labels = sentiment_labels.to(device)\n        emotion_labels = emotion_labels.to(device)\n        sarcasm_labels = sarcasm_labels.to(device)xpected 4D input (got 3D input)\n\n        bully_labels = bully_labels.to(device)\n        harmful_score_labels = harmful_score_labels.to(device)\n        target_labels = target_labels.to(device)\n        \n        optimizer.zero_grad()  # Clear gradients at the start of each batch\n        \n        # Forward pass\n        sentiment_out, emotion_out, sarcasm_out, bully_out = model(images, text_input_ids, text_attention_mask)\n        \n        # Compute loss for each task\n        loss_sentiment = loss_fn_sentiment(sentiment_out, sentiment_labels)\n        loss_emotion = loss_fn_emotion(emotion_out, emotion_labels)\n        loss_sarcasm = loss_fn_sarcasm(sarcasm_out.squeeze(), sarcasm_labels.float())  # Squeeze if necessary\n        loss_bully = loss_fn_bully(bully_out, bully_labels)\n        loss_harmful_score = loss_fn_harmful_score(harmful_score_out, harmful_score_labels)\n        loss_target = loss_fn_target(target_out, target_labels)\n        \n        # Total loss (sum or weigh the losses as needed)\n        total_loss_batch = loss_sentiment + loss_emotion + loss_sarcasm + loss_bully + loss_harmful_score + loss_target\n        \n        # Backward pass and optimization\n        total_loss_batch.backward()\n        optimizer.step()  # Update model parameters\n        \n        total_loss += total_loss_batch.item()  # Accumulate loss for the epoch\n\n    # Optionally clear cache at the end of each epoch\n    torch.cuda.empty_cache()  \n    \n    # Print the average loss for the epoch\n    avg_loss = total_loss / len(train_dataloader)\n    print(f'Epoch {epoch}, Average Loss: {avg_loss:.4f}')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Multi Bully using CLIP Model Implementation","metadata":{}},{"cell_type":"code","source":"MODELS = {\n     \"RN50\": \"https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt\",\n     \"RN101\": \"https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt\",\n     \"RN50x4\": \"https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt\",\n     \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\"\n }","metadata":{"execution":{"iopub.status.busy":"2024-11-24T13:59:12.046260Z","iopub.execute_input":"2024-11-24T13:59:12.046915Z","iopub.status.idle":"2024-11-24T13:59:12.051041Z","shell.execute_reply.started":"2024-11-24T13:59:12.046880Z","shell.execute_reply":"2024-11-24T13:59:12.050130Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"!wget {MODELS[\"ViT-B/32\"]} -O clip_model.pt","metadata":{"execution":{"iopub.status.busy":"2024-11-24T13:59:14.254761Z","iopub.execute_input":"2024-11-24T13:59:14.255372Z","iopub.status.idle":"2024-11-24T13:59:21.392047Z","shell.execute_reply.started":"2024-11-24T13:59:14.255337Z","shell.execute_reply":"2024-11-24T13:59:21.391183Z"},"trusted":true},"outputs":[{"name":"stdout","text":"--2024-11-24 13:59:15--  https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\nResolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.246.70, 2620:1ec:29:1::70\nConnecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.246.70|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 353976522 (338M) [application/octet-stream]\nSaving to: 'clip_model.pt'\n\nclip_model.pt       100%[===================>] 337.58M  65.9MB/s    in 6.0s    \n\n2024-11-24 13:59:21 (56.2 MB/s) - 'clip_model.pt' saved [353976522/353976522]\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"clip_model = torch.jit.load(\"clip_model.pt\").cuda().eval()\ninput_resolution = clip_model.input_resolution.item()\ncontext_length = clip_model.context_length.item()\nvocab_size = clip_model.vocab_size.item()\n\nprint(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in clip_model.parameters()]):,}\")\nprint(\"Input resolution:\", input_resolution)\nprint(\"Context length:\", context_length)\nprint(\"Vocab size:\", vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-11-24T09:10:14.462997Z","iopub.execute_input":"2024-11-24T09:10:14.463818Z","iopub.status.idle":"2024-11-24T09:10:15.102625Z","shell.execute_reply.started":"2024-11-24T09:10:14.463781Z","shell.execute_reply":"2024-11-24T09:10:15.101657Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Model parameters: 151,277,313\nInput resolution: 224\nContext length: 77\nVocab size: 49408\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from torchvision import transforms, models\nfrom transformers import RobertaTokenizer, RobertaModel\n\n# Load CLIP, VGG19, and Roberta models\nclip_model = torch.jit.load(\"clip_model.pt\").cuda().eval()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load VGG19 Model\nvgg19 = models.vgg19(pretrained=True).features.to(device).eval()\nresnet18 = models.resnet50(weights=\"IMAGENET1K_V1\").to(device).eval()\n\n# Load RoBERTa TOkenizer\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nroberta_model = RobertaModel.from_pretrained('roberta-base').to(device)\n\n# Image transformation for VGG19 and CLIP\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n","metadata":{"execution":{"iopub.status.busy":"2024-11-24T13:59:42.485691Z","iopub.execute_input":"2024-11-24T13:59:42.486578Z","iopub.status.idle":"2024-11-24T13:59:53.876550Z","shell.execute_reply.started":"2024-11-24T13:59:42.486544Z","shell.execute_reply":"2024-11-24T13:59:53.875579Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n100%|██████████| 548M/548M [00:02<00:00, 212MB/s] \nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 195MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92f6c992b0ce4ad8a22c3175d0a6a83f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fc6abafc759400885e411e810603165"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2c8f967fc44436fa063cec770b19fb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97a9f05009b54a09a15a917b050c003e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32f33f4ef30c4770bc5baff1d62b3a60"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f75f29001594e69bf64c6a0159f88bf"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"clip_model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ! pip install -q ftfy regex tqdm\n# ! pip install -q git+https://github.com/openai/CLIP.git","metadata":{"execution":{"iopub.status.busy":"2024-10-26T19:15:29.733521Z","iopub.execute_input":"2024-10-26T19:15:29.733913Z","iopub.status.idle":"2024-10-26T19:15:56.726491Z","shell.execute_reply.started":"2024-10-26T19:15:29.733874Z","shell.execute_reply":"2024-10-26T19:15:56.725088Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import clip\n# from PIL import Image\n\n# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# model, preprocess = clip.load(\"ViT-B/32\", device=device)\n# xpected 4D input (got 3D input)\n\n# image = preprocess(Image.open(\"/kaggle/working/bully_data/data/bully_data/45.jpg\")).unsqueeze(0).to(device)\n# text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n\n# with torch.no_grad():\n#     image_features = model.encode_image(image)\n#     text_features = model.encode_text(text)\n    \n#     logits_per_image, logits_per_text = model(image, text)\n#     probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n\n# print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]","metadata":{"execution":{"iopub.status.busy":"2024-10-26T19:17:35.025466Z","iopub.execute_input":"2024-10-26T19:17:35.026162Z","iopub.status.idle":"2024-10-26T19:17:39.482949Z","shell.execute_reply.started":"2024-10-26T19:17:35.026121Z","shell.execute_reply":"2024-10-26T19:17:39.481952Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# clip.tokenize([\"RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized\"]).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T19:18:02.735854Z","iopub.execute_input":"2024-10-26T19:18:02.736271Z","iopub.status.idle":"2024-10-26T19:18:02.747161Z","shell.execute_reply.started":"2024-10-26T19:18:02.736234Z","shell.execute_reply":"2024-10-26T19:18:02.746264Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MemeDatasetClipVGG(Dataset):\n    def __init__(self, dataframe, transform, clip_model, vgg19):\n        self.dataframe = dataframe\n        self.transform = transform\n        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n        self.roberta_model = RobertaModel.from_pretrained('roberta-base').to(device)\n        self.clip_model = clip_model\n        self.vgg19 = vgg19\n        #self.resnet = resnet50\n        \n        self.img_folder = '/kaggle/working/bully_data/data/bully_data'\n        \n        # Define label mappings\n        self.text_label_mapping = {\n            \"Bully\": 1,\n            \"Nonbully\": 0\n        }\n        \n        self.sentiment_mapping = {\n            \"Positive\":1,\n            \"Neutral\": 0,\n            \"Negative\": 2\n        }\n        \n        self.emotion_mapping = {\n            \"Disgust\": 0,\n            \"Ridicule\": 1,\n            \"Sadness\": 2,\n            \"Surprise\": 3,\n            \"Anticipation\": 4,\n            \"Angry\": 5,\n            \"Happiness\": 6,\n            \"Other\": 7,\n            \"Trust\": 8,\n            \"Fear\": 9\n        }\n        \n        self.sarcasm_mapping = {\n            \"Yes\": 1,\n            \"No\": 0\n        }\n        \n        self.harmful_score_mapping = {\n            \"Harmless\": 0,\n            \"Partially-Harmful\": 1,\n            \"Very-Harmful\": 2\n        }\n        \n        self.target_mapping = {\n            \"Individual\": 0,\n            \"Society\": 1,\n            \"Organization\": 2,\n            \"Community\": 3\n        }\n    \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, idx):\n        #print(f\"<*>Loading {idx} th Data point-----\")\n        # Load image\n        img_name = self.dataframe.iloc[idx]['Img_Name']\n        img_path = os.path.join(self.img_folder, img_name)\n        image = Image.open(img_path).convert('RGB')\n        image = self.transform(image)\n        \n        img_tensor = torch.tensor(image.unsqueeze(0)).to(device)\n        #img_tensor = self.transform(image).unsqueeze(0).to(device)  # Transform and add batch dimension\n    \n        \n        # 2. Extract image features using CLIP and VGG19\n        with torch.no_grad():\n            #print(\"Loading CLIP model\")\n            image_clip_input = self.clip_model.encode_image(img_tensor).float().to(device)  #(batch_size, 512)\n            #print(\"Loading VGG19 model\")\n            image_vgg_feature = self.vgg19(img_tensor).view(-1).float().to(device)  # Flattened VGG19 features (batch_size, 1024)\n            #print(\"|------VGG Features Done\")\n            \n        # 3. Process text with CLIP and RoBERTa\n        text = self.dataframe.iloc[idx]['Img_Text']\n        text = text.replace(\"\\n\", \"\")\n        #print(f\" Datapoint {idx}: Img_Text: {text}  \")\n        #print(\"|------IMG_Text Loaded Success\")\n        tokens = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=77)\n        #print(\"|------Tokenizer Success\")\n        input_ids = tokens['input_ids'].to(device)\n        #print(\"|------Input IDs Success\")\n        attention_mask = tokens['attention_mask'].to(device)\n        #print(\"|------Attention Mask Success\")\n        \n        #clip_tokens = clip.tokenize([text]).to(device)\n        \n\n        with torch.no_grad():\n            #print(\"Loading CLIP text Encoding......\")\n            #print(input_ids)\n            text_clip_input = self.clip_model.encode_text(input_ids).float().to(device)  #(batch_size, 512)\n            #print(\"    |------CLIP Text input Encoding Success\")\n            text_roberta_output = self.roberta_model(input_ids, attention_mask=attention_mask)\n            #print(\"    |------RoBERTa output with attention Mask Success\")\n            text_roberta_embedding = text_roberta_output.last_hidden_state[:, 0, :].float().to(device)  #(batch_size, 768)\n            #print(\"|------Roberta embedding Success\")\n        \n        # Load and tokenize text\n        #text = self.dataframe.iloc[idx]['Img_Text']\n        #inputs = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n        \n        # Get labels and apply mappings\n        sentiment_label = torch.tensor(self.sentiment_mapping[self.dataframe.iloc[idx]['Sentiment']], dtype=torch.long)\n        emotion_label = torch.tensor(self.emotion_mapping[self.dataframe.iloc[idx]['Emotion']], dtype=torch.long)\n        sarcasm_label = torch.tensor(self.sarcasm_mapping[self.dataframe.iloc[idx]['Sarcasm']], dtype=torch.float)  # Binary sarcasm\n        bully_label = torch.tensor(self.text_label_mapping[self.dataframe.iloc[idx]['Img_Label']], dtype=torch.long)  # Bully detection\n        harmful_score_label = torch.tensor(self.harmful_score_mapping[self.dataframe.iloc[idx]['Harmful_Score']], dtype=torch.long)\n        target_label = torch.tensor(self.target_mapping[self.dataframe.iloc[idx]['Target']], dtype=torch.long)\n        #print(\"|------Processed------|\")\n        # 5. Prepare the sample dictionary\n        sample = {\n            \"id\": idx,\n            \"image\": image, # 3x224x224\n            \"image_clip_input\": image_clip_input,  # CLIP image features (1x512-dim)\n            \"image_vgg_feature\": image_vgg_feature,  # VGG19 image features (flattened 1x25088-dim)\n            \"text_clip_input\": text_clip_input,  # CLIP text features (1x512-dim)\n            \"text_roberta_embedding\": text_roberta_embedding,  # RoBERTa text embedding (1x768-dim)\n            \"bully_label\": bully_label,  # Cyberbullying label\n            \"sentiment\": sentiment_label,  # Sentiment label\n            \"sarcasm\": sarcasm_label,  # Sarcasm label\n            \"emotion\": emotion_label,  # Emotion label\n            \"harmful-score\": harmful_score_label,  # Harmful score\n            \"target\": target_label  # Target variable\n        }\n        \n        #print(\"|------Sampled------|\")\n        \n        return sample","metadata":{"execution":{"iopub.status.busy":"2024-11-24T14:00:07.640627Z","iopub.execute_input":"2024-11-24T14:00:07.641227Z","iopub.status.idle":"2024-11-24T14:00:07.656722Z","shell.execute_reply.started":"2024-11-24T14:00:07.641192Z","shell.execute_reply":"2024-11-24T14:00:07.655662Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# import os\n# os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n# os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\"\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-11-24T14:00:21.128923Z","iopub.execute_input":"2024-11-24T14:00:21.129605Z","iopub.status.idle":"2024-11-24T14:00:21.133339Z","shell.execute_reply.started":"2024-11-24T14:00:21.129574Z","shell.execute_reply":"2024-11-24T14:00:21.132545Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Sanity Check for Datasets\n\nimport os\n\n# Load the dataset (assume the dataframe and image directory are available)\ndataset_clip_vgg = MemeDatasetClipVGG(df, transform, clip_model, vgg19)\n\n# Create a DataLoader for batching\ndataloader_clip_vgg = DataLoader(dataset_clip_vgg, batch_size=16, shuffle=False)\nbatch_iterator_dataloader_vgg = tqdm(dataloader_clip_vgg, desc=f\"Processing DataLoader\")\n# Iterate through the dataloader\nsample = None\nfor batch in dataloader_clip_vgg:\n    ##print(f\"Len: {len(batch)}\") \n    # Further model training steps...\n    #torch.cuda.empty_cache()\n    sample=batch\n    break\n    \nprint(\"Done\")","metadata":{"execution":{"iopub.status.busy":"2024-11-24T09:10:58.338210Z","iopub.execute_input":"2024-11-24T09:10:58.338858Z","iopub.status.idle":"2024-11-24T09:11:06.081230Z","shell.execute_reply.started":"2024-11-24T09:10:58.338823Z","shell.execute_reply":"2024-11-24T09:11:06.080281Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nProcessing DataLoader:   0%|          | 0/193 [00:00<?, ?it/s]/tmp/ipykernel_30/2376718173.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  img_tensor = torch.tensor(image.unsqueeze(0)).to(device)\n","output_type":"stream"},{"name":"stdout","text":"Done\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"print(sample.keys())\nfor k, v in sample.items():\n    print(f\"Key: {k};  --> shape: {v.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-24T09:11:06.083154Z","iopub.execute_input":"2024-11-24T09:11:06.083841Z","iopub.status.idle":"2024-11-24T09:11:06.089003Z","shell.execute_reply.started":"2024-11-24T09:11:06.083803Z","shell.execute_reply":"2024-11-24T09:11:06.087843Z"},"trusted":true},"outputs":[{"name":"stdout","text":"dict_keys(['id', 'image', 'image_clip_input', 'image_vgg_feature', 'text_clip_input', 'text_roberta_embedding', 'bully_label', 'sentiment', 'sarcasm', 'emotion', 'harmful-score', 'target'])\nKey: id;  --> shape: torch.Size([16])\nKey: image;  --> shape: torch.Size([16, 3, 224, 224])\nKey: image_clip_input;  --> shape: torch.Size([16, 1, 512])\nKey: image_vgg_feature;  --> shape: torch.Size([16, 25088])\nKey: text_clip_input;  --> shape: torch.Size([16, 1, 512])\nKey: text_roberta_embedding;  --> shape: torch.Size([16, 1, 768])\nKey: bully_label;  --> shape: torch.Size([16])\nKey: sentiment;  --> shape: torch.Size([16])\nKey: sarcasm;  --> shape: torch.Size([16])\nKey: emotion;  --> shape: torch.Size([16])\nKey: harmful-score;  --> shape: torch.Size([16])\nKey: target;  --> shape: torch.Size([16])\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"class ImageClipModel(nn.Module):\n    def __init__(self,input_dim):\n        super(ImageClipModel, self).__init__()\n        self.Linear_input = nn.Linear(input_dim, 512)\n        self.Linear_512_256 = nn.Linear(512, 256)\n        self.Linear_256_128 = nn.Linear(256, 128)\n        self.dropout = nn.Dropout(0.2)\n        self.leakyReLU = nn.LeakyReLU(0.01)\n        self.ReLU = nn.ReLU()\n        \n        \n    def forward(self, x):\n        x = self.Linear_input(x)\n        x = self.leakyReLU(x)\n        x = self.Linear_512_256(x)\n        x = self.leakyReLU(x)\n        x = self.Linear_256_128(x)\n        return x\n        ","metadata":{"execution":{"iopub.status.busy":"2024-11-24T14:00:27.469189Z","iopub.execute_input":"2024-11-24T14:00:27.469952Z","iopub.status.idle":"2024-11-24T14:00:27.475420Z","shell.execute_reply.started":"2024-11-24T14:00:27.469917Z","shell.execute_reply":"2024-11-24T14:00:27.474451Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"image_clip_model = ImageClipModel(512).to(device)\ndata_sample = dataset_clip_vgg[0]\nimage_clip_data = data_sample[\"image_clip_input\"]\nprint(image_clip_data.shape)\ny = image_clip_model(image_clip_data)\ny.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-24T09:11:16.618963Z","iopub.execute_input":"2024-11-24T09:11:16.619940Z","iopub.status.idle":"2024-11-24T09:11:16.678119Z","shell.execute_reply.started":"2024-11-24T09:11:16.619882Z","shell.execute_reply":"2024-11-24T09:11:16.677466Z"},"trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([1, 512])\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/2376718173.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  img_tensor = torch.tensor(image.unsqueeze(0)).to(device)\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 128])"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"class ImageVGGDenseModel(nn.Module):\n    def __init__(self, input_dim):\n        super(ImageVGGDenseModel, self).__init__()\n        self.Linear_input = nn.Linear(input_dim, 25088) # input_dim: 25088\n        self.Linear_25088_1024 = nn.Linear(25088, 1024)\n        self.Linear_1024_512 = nn.Linear(1024, 512)\n        self.Linear_512_256 = nn.Linear(512, 256)\n        self.Linear_256_128 = nn.Linear(256, 128)\n        self.dropout = nn.Dropout(0.2)\n        self.leakyReLU = nn.LeakyReLU(0.01)\n        self.ReLU = nn.ReLU()\n        \n        \n    def forward(self, x):\n        x = self.Linear_input(x)\n        x = self.leakyReLU(x)\n        x = self.Linear_25088_1024(x)\n        x = self.leakyReLU(x)\n        x = self.Linear_1024_512(x)\n        return x\n        ","metadata":{"execution":{"iopub.status.busy":"2024-11-24T14:00:30.284505Z","iopub.execute_input":"2024-11-24T14:00:30.285209Z","iopub.status.idle":"2024-11-24T14:00:30.290995Z","shell.execute_reply.started":"2024-11-24T14:00:30.285174Z","shell.execute_reply":"2024-11-24T14:00:30.290115Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"image_vgg_model = ImageVGGDenseModel(25088).to(device)\ndata_sample = dataset_clip_vgg[0]\nimage_vgg_data = data_sample[\"image_vgg_feature\"]\nprint(image_vgg_data.shape)\ny = image_vgg_model(image_vgg_data)\ny.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-24T09:11:22.362247Z","iopub.execute_input":"2024-11-24T09:11:22.362983Z","iopub.status.idle":"2024-11-24T09:11:28.920123Z","shell.execute_reply.started":"2024-11-24T09:11:22.362948Z","shell.execute_reply":"2024-11-24T09:11:28.919099Z"},"trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([25088])\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/2376718173.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  img_tensor = torch.tensor(image.unsqueeze(0)).to(device)\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"torch.Size([512])"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"class TextClipModel(nn.Module):\n    def __init__(self, input_dim):\n        super(TextClipModel, self).__init__()\n        self.Linear_input = nn.Linear(input_dim, 512)\n        self.Linear_512_256 = nn.Linear(512, 256)\n        self.Linear_256_128 = nn.Linear(256, 128)\n        self.dropout = nn.Dropout(0.2)\n        self.leakyReLU = nn.LeakyReLU(0.01)\n        self.ReLU = nn.ReLU()\n        \n        \n    def forward(self, x):\n        x = self.Linear_input(x)\n        x = self.leakyReLU(x)\n        x = self.Linear_512_256(x)\n        x = self.leakyReLU(x)\n        x = self.Linear_256_128(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-11-24T14:00:33.797074Z","iopub.execute_input":"2024-11-24T14:00:33.797672Z","iopub.status.idle":"2024-11-24T14:00:33.803120Z","shell.execute_reply.started":"2024-11-24T14:00:33.797639Z","shell.execute_reply":"2024-11-24T14:00:33.802242Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"text_clip_model = TextClipModel(512).to(device)\ndata_sample = dataset_clip_vgg[0]\ntext_clip_data = data_sample[\"text_clip_input\"]\nprint(text_clip_data.shape)\ny = text_clip_model(text_clip_data)\ny.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-24T09:11:32.249828Z","iopub.execute_input":"2024-11-24T09:11:32.250193Z","iopub.status.idle":"2024-11-24T09:11:32.290516Z","shell.execute_reply.started":"2024-11-24T09:11:32.250160Z","shell.execute_reply":"2024-11-24T09:11:32.289653Z"},"trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([1, 512])\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/2376718173.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  img_tensor = torch.tensor(image.unsqueeze(0)).to(device)\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 128])"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"class TextRobertaModel(nn.Module):\n    def __init__(self, input_dim):\n        super(TextRobertaModel, self).__init__()\n        self.Linear_input = nn.Linear(input_dim, 768)\n        self.Linear_768_512 = nn.Linear(768, 512)\n        self.Linear_512_128 = nn.Linear(512, 128)\n        self.Linear_512_256 = nn.Linear(512, 256)\n        self.Linear_256_128 = nn.Linear(256, 128)\n        self.dropout = nn.Dropout(0.2)\n        self.leakyReLU = nn.LeakyReLU(0.01)\n        self.ReLU = nn.ReLU()\n        \n        \n    def forward(self, x):\n        x = self.Linear_input(x)\n        x = self.leakyReLU(x)\n        x = self.Linear_768_512(x)\n        x = self.leakyReLU(x)\n        x = self.Linear_512_128(x)\n#         x = self.leakyReLU(x)\n#         x = self.Linear_256_128(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-11-24T14:00:38.277009Z","iopub.execute_input":"2024-11-24T14:00:38.277855Z","iopub.status.idle":"2024-11-24T14:00:38.283810Z","shell.execute_reply.started":"2024-11-24T14:00:38.277806Z","shell.execute_reply":"2024-11-24T14:00:38.282885Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"text_roberta_model = TextRobertaModel(768).to(device)\ndata_sample = dataset_clip_vgg[0]\ntext_roberta_data = data_sample[\"text_roberta_embedding\"]\nprint(text_roberta_data.shape)\ny = text_roberta_model(text_roberta_data)\ny.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T09:11:37.674769Z","iopub.execute_input":"2024-11-24T09:11:37.675105Z","iopub.status.idle":"2024-11-24T09:11:37.721429Z","shell.execute_reply.started":"2024-11-24T09:11:37.675075Z","shell.execute_reply":"2024-11-24T09:11:37.720503Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 768])\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/2376718173.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  img_tensor = torch.tensor(image.unsqueeze(0)).to(device)\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 128])"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# class ResNetPreModel(nn.Module):\n#     def __init__(self):\n#         super(ResNetPreModel, self).__init__()\n#         # Load pre-trained ResNet model for image processing\n#         self.image_model = models.resnet18(pretrained=True)\n#         self.image_model.fc = nn.Linear(self.image_model.fc.in_features, 512)  # Modify the final layer\n        \n#     def forward(self, image):\n#         x = self.image_model(image)\n#         return x","metadata":{"execution":{"iopub.status.busy":"2024-11-24T09:11:40.426848Z","iopub.execute_input":"2024-11-24T09:11:40.427592Z","iopub.status.idle":"2024-11-24T09:11:40.432545Z","shell.execute_reply.started":"2024-11-24T09:11:40.427557Z","shell.execute_reply":"2024-11-24T09:11:40.431650Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# resnet_model = ResNetPreModel()\n# data_sample = dataset_clip_vgg[0]\n# image = data_sample[\"image\"].unsqueeze(0)\n# print(image.shape)\n# y = resnet_model(image)\n# y.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-24T09:11:44.746695Z","iopub.execute_input":"2024-11-24T09:11:44.747532Z","iopub.status.idle":"2024-11-24T09:11:45.846496Z","shell.execute_reply.started":"2024-11-24T09:11:44.747494Z","shell.execute_reply":"2024-11-24T09:11:45.845740Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n  0%|          | 0.00/44.7M [00:00<?, ?B/s]\u001b[A\n 17%|█▋        | 7.50M/44.7M [00:00<00:00, 77.5MB/s]\u001b[A\n 36%|███▌      | 15.9M/44.7M [00:00<00:00, 83.4MB/s]\u001b[A\n 55%|█████▍    | 24.5M/44.7M [00:00<00:00, 86.4MB/s]\u001b[A\n 73%|███████▎  | 32.8M/44.7M [00:00<00:00, 80.4MB/s]\u001b[A\n100%|██████████| 44.7M/44.7M [00:00<00:00, 83.8MB/s]\u001b[A\n/tmp/ipykernel_30/2376718173.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  img_tensor = torch.tensor(image.unsqueeze(0)).to(device)\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([1, 3, 224, 224])\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 512])"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"class SelfAttentionBiLSTM(nn.Module):\n    def __init__(self, input_dim):\n        super(SelfAttentionBiLSTM, self).__init__()\n        \n        self.bilstm = nn.LSTM(input_size=input_dim, hidden_size=input_dim, bidirectional=True, batch_first=True)\n        self.fc_q = nn.Linear(1536, 512)\n        self.fc_k = nn.Linear(1536, 512)\n        self.fc_v = nn.Linear(1536, 512)\n        \n    def attention_fusion(self, vec1, vec2):\n        img_text = torch.cat((vec1, vec2), 1)\n        prob_img = torch.sigmoid(self.prob_img(img_text))\n        prob_txt = torch.sigmoid(self.prob_text(img_text))\n        \n        vec1 = prob_img * vec1\n        vec2 = prob_txt * vec2\n        \n        out_rep = torch.cat((vec1, vec2), 1)\n        \n        return out_rep \n    \n    def forward(self, x):\n        # Ensure input has batch, sequence, and feature dimensions\n        if len(x.shape) == 2:\n            x = x.unsqueeze(1)  # Add a sequence length dimension for single-time-step inputs\n        #print(f\"Input Shape after unsqueeze: {x.shape}\")\n\n        batch_size = x.shape[0]\n        #print(f\"Batch Size: {batch_size}\")\n        \n        # Initialize h0 and c0 for LSTM with batch size\n        h0 = torch.randn(2 * 1, batch_size, 768).to(x.device)  # (num_layers * num_directions, batch_size, hidden_size)\n        c0 = torch.randn(2 * 1, batch_size, 768).to(x.device)  # (num_layers * num_directions, batch_size, hidden_size)\n                \n        after_lstm = self.bilstm(x, (h0, c0))[0]\n\n        q = F.relu(self.fc_q(after_lstm))\n        k = F.relu(self.fc_k(after_lstm))\n        v = F.relu(self.fc_v(after_lstm))\n\n        att = F.tanh(torch.bmm(q, k.transpose(1, 2)))  # Transpose k for batch matmul\n        #print(\"Attention Shape :\", att.shape)\n        \n        soft = F.softmax(att, dim=-1)\n        #print(\"Softmax Shape :\", soft.shape)\n        \n        value = torch.mean(torch.bmm(soft, v), dim=1)\n        #print(\"Value Shape :\", value.shape)\n        \n        return value","metadata":{"execution":{"iopub.status.busy":"2024-11-24T14:00:45.591127Z","iopub.execute_input":"2024-11-24T14:00:45.591461Z","iopub.status.idle":"2024-11-24T14:00:45.600476Z","shell.execute_reply.started":"2024-11-24T14:00:45.591433Z","shell.execute_reply":"2024-11-24T14:00:45.599546Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"self_attention_model = SelfAttentionBiLSTM(768).to(device)\ndata_sample = dataset_clip_vgg[0]\ntext_roberta_data = data_sample[\"text_roberta_embedding\"]\nprint(text_roberta_data.shape)\ny = self_attention_model(text_roberta_data)\ny.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-24T09:11:55.803251Z","iopub.execute_input":"2024-11-24T09:11:55.804108Z","iopub.status.idle":"2024-11-24T09:11:56.063498Z","shell.execute_reply.started":"2024-11-24T09:11:55.804057Z","shell.execute_reply":"2024-11-24T09:11:56.062815Z"},"trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([1, 768])\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/2376718173.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  img_tensor = torch.tensor(image.unsqueeze(0)).to(device)\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 512])"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"class MemeModelCLIPVGG(nn.Module):\n    def __init__(self,img_clip_inp_dim, vgg_inp_dim, text_clip_inp_dim, text_roberta_inp_dim):\n        super(MemeModelCLIPVGG,self).__init__()\n        \n        self.image_clip_model = ImageClipModel(img_clip_inp_dim)\n        self.vgg_dense =  ImageVGGDenseModel(vgg_inp_dim)\n        self.text_clip_model = TextClipModel(text_clip_inp_dim)\n        self.text_roberta_model = TextRobertaModel(text_roberta_inp_dim)\n        #self.resnet_model = ResNetPreModel()\n        \n        self.attention_bilstm_model = SelfAttentionBiLSTM(text_roberta_inp_dim)\n        \n        # Load pre-trained RoBERTa model\n        #self.text_roberta_model = RobertaModel.from_pretrained('roberta-base') # ->768\n        \n        # Shared fully connected layers\n        # self.fc_shared = nn.Sequential(\n        #     nn.Linear(128 + 128 + 128 + 512 + 512, 1024), # one 512 removed as resnet is not used\n        #     nn.LeakyReLU(0.01),\n        #     nn.Dropout(0.3),\n        #     nn.Linear(1024, 512),\n        #     nn.ReLU(),\n        #     nn.Dropout(0.3),\n        # )\n\n        # Shared fully connected layers with improved initialization and normalization\n        self.fc_shared = nn.Sequential(\n            nn.Linear(128 + 128 + 128 + 512+512, 1024),  # Adjusted input dimensions\n            nn.BatchNorm1d(1024),  # Add BatchNorm to stabilize training\n            nn.LeakyReLU(negative_slope=0.01),  # LeakyReLU for smoother gradients\n            nn.Dropout(0.2),  # Reduced dropout for regularization\n            \n            nn.Linear(1024, 512),  # Next layer\n            nn.BatchNorm1d(512),  # Normalize activations\n            nn.GELU(),  # GELU for smooth non-linearity\n            nn.Dropout(0.2),  # Reduced dropout for this layer\n        )\n        \n        # Define fully connected layers for each task\n        self.fc_sentiment = nn.Linear(512, 3)  # 3 classes for sentiment\n        self.fc_emotion = nn.Linear(512, 10)  # 10 classes for emotion\n        self.fc_sarcasm = nn.Linear(512, 1)  # Binary classification for sarcasm\n        self.fc_bully = nn.Linear(512, 2)  # Binary classification for bully detection\n        self.fc_harmful_score = nn.Linear(512, 3)  # 3 classes for harmful score\n        self.fc_target = nn.Linear(512, 4)  # 4 classes for target\n        \n    \n    def forward(self, image, image_clip_input, image_vgg_feature, text_clip_input, text_roberta_embedding):\n    \n        #use BERT + VGG19 combination\n        \n        img_clip_out = self.image_clip_model(image_clip_input) # 512 ->128\n        text_clip_out = self.text_clip_model(text_clip_input) #512 ->128\n        text_roberta_out = self.text_roberta_model(text_roberta_embedding) # 768 -> 128\n        #img_resnet_out = self.resnet_model(image) # 3x224x224 -> 512\n        vgg19_out = self.vgg_dense(image_vgg_feature) #25088 -> 512 # in_CI\n        \n        self_atten_bilstm_out = self.attention_bilstm_model(text_roberta_embedding) #768 -> 512\n        #128 + 128+ 128+ 512 + 512 +512\n        \n#         print(\"img_clip_out shape:\", img_clip_out.shape)\n#         print(\"text_clip_out shape:\", text_clip_out.shape)\n#         print(\"text_roberta_out shape:\", text_roberta_out.shape)\n#         print(\"img_resnet_out shape:\", img_resnet_out.shape)\n#         print(\"vgg19_out shape:\", vgg19_out.shape)\n#         print(\"self_atten_bilstm_out shape:\", self_atten_bilstm_out.shape)\n        \n        # Add a batch dimension to vgg19_out\n        # if vgg19_out.dim() == 1:  # Check if it has only one dimension\n            # vgg19_out = vgg19_out.unsqueeze(0)  # Shape: (1, 512)\n            \n        # Squeeze the second dimension to make all tensors 2D\n        img_clip_out = img_clip_out.squeeze(1)  # Shape: (8, 128)\n        text_clip_out = text_clip_out.squeeze(1)  # Shape: (8, 128)\n        text_roberta_out = text_roberta_out.squeeze(1)  # Shape: (8, 128)\n        \n        combined_features = torch.cat((img_clip_out, text_clip_out, text_roberta_out, vgg19_out, self_atten_bilstm_out), dim = 1)\n        \n        features = self.fc_shared(combined_features)\n        \n        # Pass through fully connected layers for each task\n        sentiment_output = self.fc_sentiment(features)\n        emotion_output = self.fc_emotion(features)\n        sarcasm_output = self.fc_sarcasm(features)\n        bully_output = self.fc_bully(features)\n        harmful_score_output = self.fc_harmful_score(features)\n        target_output = self.fc_target(features)\n        \n        # Task-specific outputs with softmax or sigmoid\n#         sentiment_output = F.softmax(self.fc_sentiment(features), dim=1)  # 3-class softmax\n#         emotion_output = F.softmax(self.fc_emotion(features), dim=1)      # 10-class softmax\n#         sarcasm_output = torch.sigmoid(self.fc_sarcasm(features))         # Binary sigmoid\n#         bully_output = F.softmax(self.fc_bully(features), dim=1)          # Binary softmax\n#         harmful_score_output = F.softmax(self.fc_harmful_score(features), dim=1)  # 3-class softmax\n#         target_output = F.softmax(self.fc_target(features), dim=1)        # 4-class softmax\n        \n        return sentiment_output, emotion_output, sarcasm_output, bully_output, harmful_score_output, target_output\n                        \n    ","metadata":{"execution":{"iopub.status.busy":"2024-11-24T14:02:53.641841Z","iopub.execute_input":"2024-11-24T14:02:53.642173Z","iopub.status.idle":"2024-11-24T14:02:53.652872Z","shell.execute_reply.started":"2024-11-24T14:02:53.642145Z","shell.execute_reply":"2024-11-24T14:02:53.651835Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Sanity Check for Model \n# Check if GPU is available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\n\n#Example usage\nimg_clip_inp_dim = 512\nvgg_inp_dim = 25088\ntext_clip_inp_dim = 512\ntext_roberta_inp_dim = 768\n\nmodel = MemeModelCLIPVGG(img_clip_inp_dim, vgg_inp_dim, text_clip_inp_dim, text_roberta_inp_dim) # Create the dataset and dataloader\ndataset = MemeDataset(df, transform=transform)\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n# Create data loaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n#model= nn.DataParallel(model)\nmodel.to(device)\n\n# load dtaset and test\ndata_sample = dataset_clip_vgg[0]\nimage = data_sample['image'].unsqueeze(0).to(device) # be careful\nimage_clip_input = data_sample['image_clip_input'].to(device)\nimage_vgg_feature = data_sample['image_vgg_feature'].to(device)\ntext_clip_input = data_sample['text_clip_input'].to(device)\ntext_roberta_embedding = data_sample['text_roberta_embedding'].to(device)\n\nsentiment_output, emotion_output, sarcasm_output, bully_output, harmful_score_output, target_output = model(image, image_clip_input, image_vgg_feature, text_clip_input, text_roberta_embedding )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T09:16:17.198203Z","iopub.execute_input":"2024-11-24T09:16:17.198594Z","iopub.status.idle":"2024-11-24T09:16:23.018598Z","shell.execute_reply.started":"2024-11-24T09:16:17.198561Z","shell.execute_reply":"2024-11-24T09:16:23.017276Z"}},"outputs":[{"name":"stdout","text":"Device: cuda:0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[33], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m text_roberta_inp_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m768\u001b[39m\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m MemeModelCLIPVGG(img_clip_inp_dim, vgg_inp_dim, text_clip_inp_dim, text_roberta_inp_dim) \u001b[38;5;66;03m# Create the dataset and dataloader\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMemeDataset\u001b[49m(df, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m     14\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset))\n\u001b[1;32m     15\u001b[0m val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset) \u001b[38;5;241m-\u001b[39m train_size\n","\u001b[0;31mNameError\u001b[0m: name 'MemeDataset' is not defined"],"ename":"NameError","evalue":"name 'MemeDataset' is not defined","output_type":"error"}],"execution_count":33},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-11-24T09:21:05.386748Z","iopub.execute_input":"2024-11-24T09:21:05.387504Z","iopub.status.idle":"2024-11-24T09:21:05.391463Z","shell.execute_reply.started":"2024-11-24T09:21:05.387469Z","shell.execute_reply":"2024-11-24T09:21:05.390540Z"},"trusted":true},"outputs":[],"execution_count":41},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-10-28T07:47:52.386463Z","iopub.execute_input":"2024-10-28T07:47:52.386872Z","iopub.status.idle":"2024-10-28T07:47:53.424679Z","shell.execute_reply.started":"2024-10-28T07:47:52.386836Z","shell.execute_reply":"2024-10-28T07:47:53.423526Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-10-28T07:47:04.263699Z","iopub.execute_input":"2024-10-28T07:47:04.264511Z","iopub.status.idle":"2024-10-28T07:47:05.391957Z","shell.execute_reply.started":"2024-10-28T07:47:04.264467Z","shell.execute_reply":"2024-10-28T07:47:05.390981Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.memory_summary(device=None, abbreviated=False)","metadata":{"execution":{"iopub.status.busy":"2024-11-24T09:20:56.330008Z","iopub.execute_input":"2024-11-24T09:20:56.330449Z","iopub.status.idle":"2024-11-24T09:20:56.337267Z","shell.execute_reply.started":"2024-11-24T09:20:56.330382Z","shell.execute_reply":"2024-11-24T09:20:56.336340Z"},"trusted":true},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  15363 MiB |  15373 MiB |  34340 MiB |  18977 MiB |\\n|       from large pool |  15295 MiB |  15300 MiB |  27058 MiB |  11763 MiB |\\n|       from small pool |     68 MiB |     73 MiB |   7282 MiB |   7213 MiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  15363 MiB |  15373 MiB |  34340 MiB |  18977 MiB |\\n|       from large pool |  15295 MiB |  15300 MiB |  27058 MiB |  11763 MiB |\\n|       from small pool |     68 MiB |     73 MiB |   7282 MiB |   7213 MiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |  15332 MiB |  15341 MiB |  33905 MiB |  18573 MiB |\\n|       from large pool |  15264 MiB |  15268 MiB |  26626 MiB |  11362 MiB |\\n|       from small pool |     68 MiB |     73 MiB |   7279 MiB |   7211 MiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  15474 MiB |  15474 MiB |  15594 MiB | 122880 KiB |\\n|       from large pool |  15400 MiB |  15400 MiB |  15514 MiB | 116736 KiB |\\n|       from small pool |     74 MiB |     74 MiB |     80 MiB |   6144 KiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory | 111324 KiB | 176367 KiB |  12904 MiB |  12795 MiB |\\n|       from large pool | 107504 KiB | 174336 KiB |   5490 MiB |   5385 MiB |\\n|       from small pool |   3820 KiB |   6646 KiB |   7413 MiB |   7409 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |    2113    |    2174    |   38905    |   36792    |\\n|       from large pool |     497    |     498    |    1737    |    1240    |\\n|       from small pool |    1616    |    1676    |   37168    |   35552    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |    2113    |    2174    |   38905    |   36792    |\\n|       from large pool |     497    |     498    |    1737    |    1240    |\\n|       from small pool |    1616    |    1676    |   37168    |   35552    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     152    |     152    |     159    |       7    |\\n|       from large pool |     115    |     115    |     119    |       4    |\\n|       from small pool |      37    |      37    |      40    |       3    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      63    |      74    |   14639    |   14576    |\\n|       from large pool |      50    |      59    |     742    |     692    |\\n|       from small pool |      13    |      19    |   13897    |   13884    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"},"metadata":{}}],"execution_count":40},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"## Load Dtaset and Create the Model","metadata":{}},{"cell_type":"code","source":"torch.cuda.device_count()","metadata":{"execution":{"iopub.status.busy":"2024-10-28T16:55:46.853246Z","iopub.execute_input":"2024-10-28T16:55:46.853916Z","iopub.status.idle":"2024-10-28T16:55:46.860469Z","shell.execute_reply.started":"2024-10-28T16:55:46.853877Z","shell.execute_reply":"2024-10-28T16:55:46.859562Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the dataset (assume the dataframe and image directory are available)\ndataset_clip_vgg = MemeDatasetClipVGG(df, transform, clip_model, vgg19)\n\ntrain_size = int(0.8 * len(dataset_clip_vgg))\nval_size = len(dataset_clip_vgg) - train_size\n\n# Split Dataset\ntrain_dataset, val_dataset = random_split(dataset_clip_vgg, [train_size, val_size])\n\n# Create data loaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n\n# Initialize the model and move it to the GPU\n\nimg_clip_inp_dim = 512\nvgg_inp_dim = 25088\ntext_clip_inp_dim = 512\ntext_roberta_inp_dim = 768\n\nmodel = MemeModelCLIPVGG(img_clip_inp_dim, vgg_inp_dim, text_clip_inp_dim, text_roberta_inp_dim)\n\n# Parallelize\n# if torch.cuda.device_count() > 1:\n#     model = nn.DataParallel(model)  # Use DataParallel if multiple GPUs are available\n\nmodel.to(device) \n\n# Initialize the Optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-24T14:03:14.854903Z","iopub.execute_input":"2024-11-24T14:03:14.855747Z","iopub.status.idle":"2024-11-24T14:03:22.057527Z","shell.execute_reply.started":"2024-11-24T14:03:14.855700Z","shell.execute_reply":"2024-11-24T14:03:22.056833Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Make the Loss Functions\n# Loss functions\nloss_fn_sentiment = nn.CrossEntropyLoss().to(device)\nloss_fn_emotion = nn.CrossEntropyLoss().to(device)\nloss_fn_sarcasm = nn.BCEWithLogitsLoss().to(device)\nloss_fn_bully = nn.CrossEntropyLoss().to(device)\nloss_fn_harmful_score = nn.CrossEntropyLoss().to(device)\nloss_fn_target = nn.CrossEntropyLoss().to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-24T14:03:26.549260Z","iopub.execute_input":"2024-11-24T14:03:26.549945Z","iopub.status.idle":"2024-11-24T14:03:26.554707Z","shell.execute_reply.started":"2024-11-24T14:03:26.549913Z","shell.execute_reply":"2024-11-24T14:03:26.553817Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def train_model(model, train_dataloader, val_dataloader, optimizer, losses, vals, num_epochs=10):\n    torch.cuda.empty_cache()\n    #tqdm(range(num_epochs), position=0, leave=True)\n    for epoch in range(num_epochs):\n        torch.cuda.empty_cache()\n        model.train()\n        train_running_loss = 0.0\n        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n        for data_sample in batch_iterator:\n            #extract features\n            image = data_sample['image'].to(device)\n            image_clip_input = data_sample['image_clip_input'].to(device)\n            image_vgg_feature = data_sample['image_vgg_feature'].to(device)\n            text_clip_input = data_sample['text_clip_input'].to(device)\n            text_roberta_embedding = data_sample['text_roberta_embedding'].to(device)\n\n            #extract lables\n            sentiment_labels = data_sample['sentiment'].to(device)\n            emotion_labels = data_sample['emotion'].to(device)\n            sarcasm_labels = data_sample['sarcasm'].to(device)\n            bully_labels = data_sample['bully_label'].to(device)\n            harmful_score_labels = data_sample['harmful-score'].to(device)\n            target_labels = data_sample['target'].to(device)\n\n            # Zero the parameter gradients\n            optimizer.zero_grad()\n\n            # Forward Pass\n            sentiment_output, emotion_output, sarcasm_output, bully_output, harmful_score_output, target_output = model(image, image_clip_input, image_vgg_feature, text_clip_input, text_roberta_embedding )\n\n            # Calculate Loss\n            # Compute the losses\n            loss_sentiment = loss_fn_sentiment(sentiment_output, sentiment_labels)\n            loss_emotion = loss_fn_emotion(emotion_output, emotion_labels)\n            loss_sarcasm = loss_fn_sarcasm(sarcasm_output.squeeze(), sarcasm_labels.float())\n            loss_bully = loss_fn_bully(bully_output, bully_labels)\n            loss_harmful_score = loss_fn_harmful_score(harmful_score_output, harmful_score_labels)\n            loss_target = loss_fn_target(target_output, target_labels)\n\n            # Combine the losses\n            loss = loss_sentiment + loss_emotion + loss_sarcasm + loss_bully + loss_harmful_score + loss_target\n\n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n\n            # Update running loss\n            train_running_loss += loss.item()\n        \n        # Print the average loss for this epoch\n        train_running_loss = train_running_loss / len(train_dataloader)\n        losses.append(train_running_loss)\n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {train_running_loss:.4f}')\n    \n        \n            ","metadata":{"execution":{"iopub.status.busy":"2024-11-24T14:03:30.175282Z","iopub.execute_input":"2024-11-24T14:03:30.175610Z","iopub.status.idle":"2024-11-24T14:03:30.184809Z","shell.execute_reply.started":"2024-11-24T14:03:30.175581Z","shell.execute_reply":"2024-11-24T14:03:30.183770Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import os\n\n#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]= \"expandable_segments:True\"","metadata":{"execution":{"iopub.status.busy":"2024-10-28T11:20:14.748479Z","iopub.execute_input":"2024-10-28T11:20:14.749210Z","iopub.status.idle":"2024-10-28T11:20:14.753975Z","shell.execute_reply.started":"2024-10-28T11:20:14.749164Z","shell.execute_reply":"2024-10-28T11:20:14.752843Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"losses = []\nvals = []\ntrain_model(model, train_dataloader, val_dataloader, optimizer, losses, vals, num_epochs=20)","metadata":{"execution":{"iopub.status.busy":"2024-11-24T14:03:40.431614Z","iopub.execute_input":"2024-11-24T14:03:40.432315Z","iopub.status.idle":"2024-11-24T14:35:04.920900Z","shell.execute_reply.started":"2024-11-24T14:03:40.432279Z","shell.execute_reply":"2024-11-24T14:35:04.920014Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Processing Epoch 00:   0%|          | 0/308 [00:00<?, ?it/s]/tmp/ipykernel_30/2376718173.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  img_tensor = torch.tensor(image.unsqueeze(0)).to(device)\nProcessing Epoch 00: 100%|██████████| 308/308 [01:39<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20, Loss: 5.1323\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 01: 100%|██████████| 308/308 [01:33<00:00,  3.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/20, Loss: 4.3281\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 02: 100%|██████████| 308/308 [01:34<00:00,  3.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/20, Loss: 3.7470\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 03: 100%|██████████| 308/308 [01:35<00:00,  3.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/20, Loss: 2.9430\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 04: 100%|██████████| 308/308 [01:33<00:00,  3.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/20, Loss: 2.1904\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 05: 100%|██████████| 308/308 [01:32<00:00,  3.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/20, Loss: 1.7072\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 06: 100%|██████████| 308/308 [01:32<00:00,  3.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/20, Loss: 1.4140\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 07: 100%|██████████| 308/308 [01:32<00:00,  3.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/20, Loss: 1.2443\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 08: 100%|██████████| 308/308 [01:33<00:00,  3.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/20, Loss: 1.0635\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 09: 100%|██████████| 308/308 [01:35<00:00,  3.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/20, Loss: 0.9863\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 10: 100%|██████████| 308/308 [01:33<00:00,  3.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/20, Loss: 0.8924\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 11: 100%|██████████| 308/308 [01:32<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/20, Loss: 0.8295\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 12: 100%|██████████| 308/308 [01:33<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/20, Loss: 0.8021\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 13: 100%|██████████| 308/308 [01:35<00:00,  3.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/20, Loss: 0.7220\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 14: 100%|██████████| 308/308 [01:34<00:00,  3.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/20, Loss: 0.7220\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 15: 100%|██████████| 308/308 [01:33<00:00,  3.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/20, Loss: 0.7278\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 16: 100%|██████████| 308/308 [01:33<00:00,  3.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/20, Loss: 0.7159\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 17: 100%|██████████| 308/308 [01:34<00:00,  3.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/20, Loss: 0.6195\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 18: 100%|██████████| 308/308 [01:34<00:00,  3.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/20, Loss: 0.6003\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 19: 100%|██████████| 308/308 [01:33<00:00,  3.29it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 20/20, Loss: 0.5952\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# Import required libraries (if not already done)\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Validation\nmodel.eval()\nval_running_loss = 0.0\n\n# Initialize lists to store true and predicted labels for each task\nall_labels_bully = []\nall_preds_bully = []\n\nall_labels_sentiment = []\nall_preds_sentiment = []\n\nall_labels_emotion = []\nall_preds_emotion = []\n\nall_labels_sarcasm = []\nall_preds_sarcasm = []\n\nall_labels_harmful_score = []\nall_preds_harmful_score = []\n\nall_labels_target = []\nall_preds_target = []\n\nepoch = 20\n\nwith torch.no_grad():\n    torch.cuda.empty_cache()\n    batch_iterator = tqdm(val_dataloader, desc=f\"Processing Validation {epoch:02d}\")\n    for data_sample in batch_iterator:\n        #extract features\n        image = data_sample['image'].to(device)\n        image_clip_input = data_sample['image_clip_input'].to(device)\n        image_vgg_feature = data_sample['image_vgg_feature'].to(device)\n        text_clip_input = data_sample['text_clip_input'].to(device)\n        text_roberta_embedding = data_sample['text_roberta_embedding'].to(device)\n\n        #extract lables\n        sentiment_labels = data_sample['sentiment'].to(device)\n        emotion_labels = data_sample['emotion'].to(device)\n        sarcasm_labels = data_sample['sarcasm'].to(device)\n        bully_labels = data_sample['bully_label'].to(device)\n        harmful_score_labels = data_sample['harmful-score'].to(device)\n        target_labels = data_sample['target'].to(device)\n        \n        # Forward Pass\n        sentiment_output, emotion_output, sarcasm_output, bully_output, harmful_score_output, target_output = model(image, image_clip_input, image_vgg_feature, text_clip_input, text_roberta_embedding )\n\n        # Calculate Loss\n        # Compute the losses\n        loss_sentiment = loss_fn_sentiment(sentiment_output, sentiment_labels)\n        loss_emotion = loss_fn_emotion(emotion_output, emotion_labels)\n        loss_sarcasm = loss_fn_sarcasm(sarcasm_output.squeeze(), sarcasm_labels.float())\n        loss_bully = loss_fn_bully(bully_output, bully_labels)\n        loss_harmful_score = loss_fn_harmful_score(harmful_score_output, harmful_score_labels)\n        loss_target = loss_fn_target(target_output, target_labels)\n        \n        # Combine the losses\n        loss = loss_sentiment + loss_emotion + loss_sarcasm + loss_bully + loss_harmful_score + loss_target\n        \n        # Update running loss\n        val_running_loss += loss.item()\n\n        # Get predictions for each task\n        _, predicted_sentiment = torch.max(sentiment_output, 1)\n        _, predicted_emotion = torch.max(emotion_output, 1)\n        _, predicted_sarcasm = torch.max(sarcasm_output, 1)\n        _, predicted_bully = torch.max(bully_output, 1)\n        _, predicted_harmful_score = torch.max(harmful_score_output, 1)  # Assuming multi-class\n        _, predicted_target = torch.max(target_output, 1)  # Assuming multi-class\n\n        # Collect true and predicted labels for each task\n        all_labels_sentiment.append(sentiment_labels.cpu().numpy())\n        all_preds_sentiment.append(predicted_sentiment.cpu().numpy())\n\n        all_labels_emotion.append(emotion_labels.cpu().numpy())\n        all_preds_emotion.append(predicted_emotion.cpu().numpy())\n\n        all_labels_sarcasm.append(sarcasm_labels.cpu().numpy())\n        all_preds_sarcasm.append(predicted_sarcasm.cpu().numpy())\n\n        all_labels_bully.append(bully_labels.cpu().numpy())\n        all_preds_bully.append(predicted_bully.cpu().numpy())\n\n        all_labels_harmful_score.append(harmful_score_labels.cpu().numpy())\n        all_preds_harmful_score.append(predicted_harmful_score.cpu().numpy())\n\n        all_labels_target.append(target_labels.cpu().numpy())\n        all_preds_target.append(predicted_target.cpu().numpy())\n        \n    # Print the average loss for this epoch\n    val_running_loss = val_running_loss / len(train_dataloader)\n    vals.append(val_running_loss)\n    print(f'Epoch 20, Val_Loss: {val_running_loss:.4f}')\n\n\n# Flatten lists for each task\nall_labels_bully = np.concatenate(all_labels_bully)\nall_preds_bully = np.concatenate(all_preds_bully)\n\nall_labels_sentiment = np.concatenate(all_labels_sentiment)\nall_preds_sentiment = np.concatenate(all_preds_sentiment)\n\nall_labels_emotion = np.concatenate(all_labels_emotion)\nall_preds_emotion = np.concatenate(all_preds_emotion)\n\nall_labels_sarcasm = np.concatenate(all_labels_sarcasm)\nall_preds_sarcasm = np.concatenate(all_preds_sarcasm)\n\nall_labels_harmful_score = np.concatenate(all_labels_harmful_score)\nall_preds_harmful_score = np.concatenate(all_preds_harmful_score)\n\nall_labels_target = np.concatenate(all_labels_target)\nall_preds_target = np.concatenate(all_preds_target)\n\n# Calculate accuracy and F1 score for each task\naccuracy_bully_SA_EM = accuracy_score(all_labels_bully, all_preds_bully)\nf1_bully_SA_EM = f1_score(all_labels_bully, all_preds_bully, average='weighted')\n\naccuracy_sentiment_SA_EM = accuracy_score(all_labels_sentiment, all_preds_sentiment)\nf1_sentiment_SA_EM = f1_score(all_labels_sentiment, all_preds_sentiment, average='weighted')\n\naccuracy_emotion_SA_EM = accuracy_score(all_labels_emotion, all_preds_emotion)\nf1_emotion_SA_EM = f1_score(all_labels_emotion, all_preds_emotion, average='weighted')\n\naccuracy_sarcasm_SA_EM = accuracy_score(all_labels_sarcasm, all_preds_sarcasm)\nf1_sarcasm_SA_EM = f1_score(all_labels_sarcasm, all_preds_sarcasm, average='weighted')\n\naccuracy_harmful_score_SA_EM = accuracy_score(all_labels_harmful_score, all_preds_harmful_score)\nf1_harmful_score_SA_EM = f1_score(all_labels_harmful_score, all_preds_harmful_score, average='weighted')\n\naccuracy_target_SA_EM = accuracy_score(all_labels_target, all_preds_target)\nf1_target_SA_EM = f1_score(all_labels_target, all_preds_target, average='weighted')\n\nprint(f'Epoch {epoch}, Validation Loss: {val_running_loss:.4f},\\n'\n      f'Bully Accuracy: {accuracy_bully_SA_EM:.4f}, F1 Score: {f1_bully_SA_EM:.4f},\\n'\n      f'Sentiment Accuracy: {accuracy_sentiment_SA_EM:.4f}, F1 Score: {f1_sentiment_SA_EM:.4f},\\n'\n      f'Emotion Accuracy: {accuracy_emotion_SA_EM:.4f}, F1 Score: {f1_emotion_SA_EM:.4f},\\n'\n      f'Sarcasm Accuracy: {accuracy_sarcasm_SA_EM:.4f}, F1 Score: {f1_sarcasm_SA_EM:.4f},\\n'\n      f'Harmful Score Accuracy: {accuracy_harmful_score_SA_EM:.4f}, F1 Score: {f1_harmful_score_SA_EM:.4f},\\n'\n      f'Target Accuracy: {accuracy_target_SA_EM:.4f}, F1 Score: {f1_target_SA_EM:.4f}')\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T14:38:38.787051Z","iopub.execute_input":"2024-11-24T14:38:38.787660Z","iopub.status.idle":"2024-11-24T14:38:54.236028Z","shell.execute_reply.started":"2024-11-24T14:38:38.787613Z","shell.execute_reply":"2024-11-24T14:38:54.235143Z"}},"outputs":[{"name":"stderr","text":"Processing Validation 20:   0%|          | 0/77 [00:00<?, ?it/s]/tmp/ipykernel_30/2376718173.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  img_tensor = torch.tensor(image.unsqueeze(0)).to(device)\nProcessing Validation 20: 100%|██████████| 77/77 [00:15<00:00,  5.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20, Val_Loss: 2.1742\nEpoch 20, Validation Loss: 2.1742,\nBully Accuracy: 0.7841, F1 Score: 0.7491,\nSentiment Accuracy: 0.6640, F1 Score: 0.6392,\nEmotion Accuracy: 0.2532, F1 Score: 0.2359,\nSarcasm Accuracy: 0.5016, F1 Score: 0.3351,\nHarmful Score Accuracy: 0.9821, F1 Score: 0.9813,\nTarget Accuracy: 0.7630, F1 Score: 0.7145\n","output_type":"stream"}],"execution_count":30}]}